Lesson:  How to Add a Hadoop Service to your Spring MVC Web App
--------------------------------------------------------------
This lesson describes how to add a Hadoop Client and
demonstrates how a Spring MVC controller could pull a file from Hadoop and return it

  
Assumptions:  
 A) You have a Spring MVC web app already setup


References
----------
http://stackoverflow.com/questions/17072543/reading-hdfs-and-local-files-in-java
http://stackoverflow.com/questions/30124206/getting-files-in-hadoop-into-a-web-application
http://www.folkstalk.com/2013/07/java-program-to-list-contents-of.html



Part 1:  Setup a HDFS and insert a file into it
------------------------------------------------
 1. Install ZooKeeper and Hadoop
    -- The easier way to is to use the Accumulo Quick-install
       which will setup a ZooKeeper, Hadoop File System, and Accumulo Instance
       [see learnAccumulo / howToSetupAccumuloQuickInstall ]
 
 
 2. If your hadoop HDFS is running on a different box, then enable remote connections
    a. Edit hdfs-site.xml
		  <property>
		    <name>dfs.namenode.rpc-bind-host</name>
		    <value>0.0.0.0</value>
		  </property>
    
    
 
 3. Start-up your Hadoop File System
    unix> source 
    unix> qi-start
    
    -- Now, you have a running hadoop file system
          
 

    
 4. Add a file to your Hadoop File System
    a. Create a directory in the HDFS called /tmp/input
       unix> hadoop fs -mkdir -p /tmp/input
    
    b. Create a file called /home/adam/stuff.txt with this 
       unix vi /home/adam/stuff.txt
            hi mom
            hi Ben
            hi Peter
            hi Dad
            hi Will
            hi Sam
            Will is cool
    
    c. Put that file in the /tmp/input direcory in HDFS
       unix> hadoop fs -put /home/adam/stuff.txt /tmp/input/stuff.txt 
    
    
    d. Verify that it is there
       unix> hadoop fs -ls -R /tmp
			drwxr-xr-x   -  adam supergroup          0 2017-01-14 00:04 /tmp/input
			-rw-r--r--   1  adam supergroup        142 2017-01-14 00:04 /tmp/input/stuff.txt

 


Part 2:  Create Java Program that talks with HDFS
-------------------------------------------------
ASSUMPTION:  Your Intellij is running on the same box as the HDFS
 
 1. Create a Java JAR program
    [see learnJava / howToCreateJavaCommandLineProgramUsingIntellijMaven.txt]
    
 
 2. Add these dependencies to your pom.xml
 
   <dependency>
      <!-- Hadoop uses log4j so use SLF4j w/log4j bridge -->
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-log4j12</artifactId>
      <version>1.7.12</version>
    </dependency>

    <dependency>
      <!-- Using the Hadoop 2.6 libraries -->
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-common</artifactId>
      <version>2.6.0</version>
      
      <exclusions>
        <exclusion>
          <groupId>org.slf4j</groupId>
          <artifactId>slf4j-log4j12</artifactId>
        </exclusion>
      </exclusions>
    </dependency>

    <dependency>
      <!-- Using the Hadoop HDFS 2.6 libraries -->
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-hdfs</artifactId>
      <version>2.6.0</version>
    </dependency>
 

 3. Delete the logback.xml file
 
 4. Create this file:  log4j.xml
    a. Create this file here:  /src/main/resources/log4j.xml
    b. Copy the following to your log4j.xml
    
		<?xml version="1.0" encoding="UTF-8" ?>
		<!DOCTYPE log4j:configuration SYSTEM "log4j.dtd">
		
		<log4j:configuration xmlns:log4j="http://jakarta.apache.org/log4j/">
		    <appender name="CONSOLE" class="org.apache.log4j.ConsoleAppender">
		        <param name="Target" value="System.out"/>
		        <layout class="org.apache.log4j.PatternLayout">
		            <param name="ConversionPattern" value="%d{MM/dd/yyyy HH:mm:ss} %-5p %t %c %m%n"/>
		        </layout>
		    </appender>
		
		
		    <logger name="com.resnick" additivity="false">
		        <level value="debug" />
		        <appender-ref ref="CONSOLE" />
		    </logger>
		
		    <logger name="org.apache.hadoop" additivity="false">
		        <level value="debug" />
		        <appender-ref ref="CONSOLE" />
		    </logger>
		
		    <root>
		        <priority value="info" />
		        <appender-ref ref="CONSOLE" />
		    </root>
		
		</log4j:configuration>

    
 5. Create this App class
    ASSUMPTION:  You have a local HDFS listening on localhost:9000
    
		package com.resnick;
		
		import org.apache.hadoop.conf.Configuration;
		import org.apache.hadoop.fs.FileSystem;
		import org.apache.hadoop.fs.Path;
		import org.apache.hadoop.io.IOUtils;
		import org.slf4j.Logger;
		import org.slf4j.LoggerFactory;
		
		import java.io.InputStream;
		import java.net.URI;
		
		
		/**
		 * Sample Class that talks to HDFS, reads a HDFS file, and displays it
		 *
		 */
		public class App
		{
		    private static final Logger logger = LoggerFactory.getLogger(App.class);
		
		    public static void main( String[] args ) throws Exception
		    {
		        logger.debug("main() started.");
		
		        System.setProperty("hadoop.home.dir", "/home/adam/quickinstall-home/hadoop-2.4.1");
		
		        // The prefix 'hdfs://localhost:9000' should match the fs.default.name property (found in core-site.xml)
		        final String sFilePath = "hdfs://localhost:9000/tmp/input/stuff.txt";
		
		        Configuration conf = new Configuration();
		        FileSystem fs = FileSystem.get(URI.create(sFilePath), conf);
		        InputStream in = null;
		        try
		        {
		            in = fs.open(new Path(sFilePath));
		
		            // Use Hadoop's IOUtils.copyBytes() to send the file to STDOUT
		            // NOTE:  The false argument tells copyBytes to not close the file.
		            IOUtils.copyBytes(in, System.out, 4096, false);
		        }
		        finally
		        {
		            IOUtils.closeStream(in);
		        }
		
		
		        logger.debug("main() finished.");
		    }
		}

 
              
 6. Debug the program
    a. In Intellij, Set a breakpoint in the main()
    b. In Intellij, right-click in this App.class -> Debug 'App.main()'
    c. You should see the following:
    
		01/14/2017 21:59:27 DEBUG main com.resnick.App main() started.
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.metrics2.lib.MutableMetricsFactory field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.metrics2.lib.MutableMetricsFactory field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.metrics2.lib.MutableMetricsFactory field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.metrics2.impl.MetricsSystemImpl UgiMetrics, User and group related metrics
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.security.Groups  Creating new Groups object
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.util.NativeCodeLoader Trying to load the custom-built native-hadoop library...
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.util.NativeCodeLoader Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.util.NativeCodeLoader java.library.path=/opt/idea-IU-143.1821.5/bin::/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
		01/14/2017 21:59:27 WARN  main org.apache.hadoop.util.NativeCodeLoader Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.util.PerformanceAdvisory Falling back to shell based
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.util.Shell setsid exited with exit code 0
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.security.Groups Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.security.UserGroupInformation hadoop login
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.security.UserGroupInformation hadoop login commit
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.security.UserGroupInformation using local user:UnixPrincipal: adam
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.security.UserGroupInformation Using user: "UnixPrincipal: adam" with name adam
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.security.UserGroupInformation User entry: "adam"
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.security.UserGroupInformation UGI loginUser:adam (auth:SIMPLE)
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.hdfs.BlockReaderLocal dfs.client.use.legacy.blockreader.local = false
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.hdfs.BlockReaderLocal dfs.client.read.shortcircuit = false
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.hdfs.BlockReaderLocal dfs.client.domain.socket.data.traffic = false
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.hdfs.BlockReaderLocal dfs.domain.socket.path = 
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.hdfs.DFSClient No KeyProvider found.
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.io.retry.RetryUtils multipleLinearRandomRetry = null
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.ipc.Server rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@501edcf1
		01/14/2017 21:59:27 DEBUG main org.apache.hadoop.ipc.Client getting client out of cache: org.apache.hadoop.ipc.Client@16e7dcfd
		01/14/2017 21:59:28 DEBUG main org.apache.hadoop.util.PerformanceAdvisory Both short-circuit local reads and UNIX domain socket are disabled.
		01/14/2017 21:59:28 DEBUG main org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
		01/14/2017 21:59:28 DEBUG main org.apache.hadoop.ipc.Client The ping interval is 60000 ms.
		01/14/2017 21:59:28 DEBUG main org.apache.hadoop.ipc.Client Connecting to localhost/127.0.0.1:9000
		01/14/2017 21:59:28 DEBUG IPC Client (1027007693) connection to localhost/127.0.0.1:9000 from adam org.apache.hadoop.ipc.Client IPC Client (1027007693) connection to localhost/127.0.0.1:9000 from adam: starting, having connections 1
		01/14/2017 21:59:28 DEBUG IPC Parameter Sending Thread #0 org.apache.hadoop.ipc.Client IPC Client (1027007693) connection to localhost/127.0.0.1:9000 from adam sending #0
		01/14/2017 21:59:28 DEBUG IPC Client (1027007693) connection to localhost/127.0.0.1:9000 from adam org.apache.hadoop.ipc.Client IPC Client (1027007693) connection to localhost/127.0.0.1:9000 from adam got value #0
		01/14/2017 21:59:28 DEBUG main org.apache.hadoop.ipc.ProtobufRpcEngine Call: getBlockLocations took 33ms
		01/14/2017 21:59:28 DEBUG main org.apache.hadoop.hdfs.DFSClient newInfo = LocatedBlocks{
		  fileLength=142
		  underConstruction=false
		  blocks=[LocatedBlock{BP-979332524-127.0.0.1-1466551605417:blk_1073742201_1381; getBlockSize()=142; corrupt=false; offset=0; locs=[127.0.0.1:50010]; storageIDs=[DS-516b443c-fb2a-4c1f-80f8-7076cf6ae0d5]; storageTypes=[DISK]}]
		  lastLocatedBlock=LocatedBlock{BP-979332524-127.0.0.1-1466551605417:blk_1073742201_1381; getBlockSize()=142; corrupt=false; offset=0; locs=[127.0.0.1:50010]; storageIDs=[DS-516b443c-fb2a-4c1f-80f8-7076cf6ae0d5]; storageTypes=[DISK]}
		  isLastBlockComplete=true}
		01/14/2017 21:59:28 DEBUG main org.apache.hadoop.hdfs.DFSClient Connecting to datanode 127.0.0.1:50010
		01/14/2017 21:59:28 DEBUG IPC Parameter Sending Thread #0 org.apache.hadoop.ipc.Client IPC Client (1027007693) connection to localhost/127.0.0.1:9000 from adam sending #1
		01/14/2017 21:59:28 DEBUG IPC Client (1027007693) connection to localhost/127.0.0.1:9000 from adam org.apache.hadoop.ipc.Client IPC Client (1027007693) connection to localhost/127.0.0.1:9000 from adam got value #1
		01/14/2017 21:59:28 DEBUG main org.apache.hadoop.ipc.ProtobufRpcEngine Call: getServerDefaults took 2ms
		01/14/2017 21:59:28 DEBUG main org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient SASL client skipping handshake in unsecured configuration for addr = /127.0.0.1, datanodeId = 127.0.0.1:50010
		           hi mom
		            hi Ben
		            hi Peter
		            hi Dad
		            hi Will
		            hi Sam
		            Will is cool
		
		01/14/2017 21:59:28 DEBUG main com.resnick.App main() finished.
		01/14/2017 21:59:28 DEBUG Thread-2 org.apache.hadoop.ipc.Client stopping client from cache: org.apache.hadoop.ipc.Client@16e7dcfd
		01/14/2017 21:59:28 DEBUG Thread-2 org.apache.hadoop.ipc.Client removing client from cache: org.apache.hadoop.ipc.Client@16e7dcfd
		01/14/2017 21:59:28 DEBUG Thread-2 org.apache.hadoop.ipc.Client stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@16e7dcfd
		01/14/2017 21:59:28 DEBUG Thread-2 org.apache.hadoop.ipc.Client Stopping client
		01/14/2017 21:59:28 DEBUG IPC Client (1027007693) connection to localhost/127.0.0.1:9000 from adam org.apache.hadoop.ipc.Client IPC Client (1027007693) connection to localhost/127.0.0.1:9000 from adam: closed
		01/14/2017 21:59:28 DEBUG IPC Client (1027007693) connection to localhost/127.0.0.1:9000 from adam org.apache.hadoop.ipc.Client IPC Client (1027007693) connection to localhost/127.0.0.1:9000 from adam: stopped, remaining connections 0
		
		Process finished with exit code 0
		    
    
    
Part 3:  Add a HadoopClient to your Java Spring MVC Web App
------------------------------------------------------------

INCOMPLETE