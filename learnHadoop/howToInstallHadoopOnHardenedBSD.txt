How to Install Hadoop on Hardened BSD
-------------------------------------

Assumptions:
 A) You are root
 B) Your operating system is Hardened BSD
 C) You want to setup a HDFS on the computer
 
 
References
----------
http://www.michael-noll.com/blog/2011/04/09/benchmarking-and-stress-testing-an-hadoop-cluster-with-terasort-testdfsio-nnbench-mrbench/
http://stackoverflow.com/questions/31733313/how-to-best-run-hadoop-on-single-machine

 
Procedures
----------
 1. Install Java 7 JDK
    unix> sudo -i    # change over as root
    
    unix> pkg search "^openjdk"   
          openjdk-7.95.00,1              Java Development Kit 7
          openjdk-jre-7.95.00,1          Java Runtime Environment 7
          openjdk6-b38,1                 Oracle's Java 6 virtual machine release under the GPL v2
          openjdk6-jre-b38,1             Oracle's Java 6 Runtime Environment under the GPL v2
          openjdk8-8.72.15               Java Development Kit 8
          openjdk8-jre-8.72.15           Java Runtime Environment 8

    unix> pkg install openjdk-7.95.00,1
          Proceed with this action? [y/N]:  y  <enter>
          
          [1/31] Installing xproto-7.0.28...
          [1/31] Extracting xproto-7.0.28: 100%
          [2/31] Installing libXdmcp-1.1.2...
          [2/31] Extracting libXdmcp-1.1.2: 100%
          [3/31] Installing libxml2-2.9.3...
          [3/31] Extracting libxml2-2.9.3: 100%
          [4/31] Installing libpthread-stubs-0.3_6...
          [4/31] Extracting libpthread-stubs-0.3_6: 100%
          [5/31] Installing libXau-1.0.8_3...
          [5/31] Extracting libXau-1.0.8_3: 100%
          [6/31] Installing libxcb-1.11.1...
          [6/31] Extracting libxcb-1.11.1: 100%
          [7/31] Installing kbproto-1.0.7...
          [7/31] Extracting kbproto-1.0.7: 100%
          [8/31] Installing libX11-1.6.3,1...
          [8/31] Extracting libX11-1.6.3,1: 100%
          [9/31] Installing xextproto-7.3.0...
          [9/31] Extracting xextproto-7.3.0: 100%
          [10/31] Installing fixesproto-5.0...
          [10/31] Extracting fixesproto-5.0: 100%
          [11/31] Installing freetype2-2.6.3...
          [11/31] Extracting freetype2-2.6.3: 100%
          [12/31] Installing libfontenc-1.1.3...
          [12/31] Extracting libfontenc-1.1.3: 100%
          [13/31] Installing libICE-1.0.9_1,1...
          [13/31] Extracting libICE-1.0.9_1,1: 100%
          [14/31] Installing inputproto-2.3.1...
          [14/31] Extracting inputproto-2.3.1: 100%
          [15/31] Installing libXext-1.3.3_1,1...
          [15/31] Extracting libXext-1.3.3_1,1: 100%
          [16/31] Installing libXfixes-5.0.1_3...
          [16/31] Extracting libXfixes-5.0.1_3: 100%
          [17/31] Installing mkfontscale-1.1.2...
          [17/31] Extracting mkfontscale-1.1.2: 100%
          [18/31] Installing libSM-1.2.2_3,1...
          [18/31] Extracting libSM-1.2.2_3,1: 100%
          [19/31] Installing recordproto-1.14.2...
          [19/31] Extracting recordproto-1.14.2: 100%
          [20/31] Installing libXi-1.7.6,1...
          [20/31] Extracting libXi-1.7.6,1: 100%
          [21/31] Installing fontconfig-2.11.1_1,1...
          [21/31] Extracting fontconfig-2.11.1_1,1: 100%
          Running fc-cache to build fontconfig cache...
          /usr/local/share/fonts: skipping, no such directory
          /usr/local/lib/X11/fonts: skipping, no such directory
          /root/.local/share/fonts: skipping, no such directory
          /root/.fonts: skipping, no such directory
          /var/db/fontconfig: cleaning cache directory
          /root/.cache/fontconfig: not cleaning non-existent cache directory
          /root/.fontconfig: not cleaning non-existent cache directory
          fc-cache: succeeded
          [22/31] Installing mkfontdir-1.0.7...
          [22/31] Extracting mkfontdir-1.0.7: 100%
          [23/31] Installing renderproto-0.11.1...
          [23/31] Extracting renderproto-0.11.1: 100%
          [24/31] Installing libXt-1.1.5,1...
          [24/31] Extracting libXt-1.1.5,1: 100%
          [25/31] Installing libXtst-1.2.2_3...
          [25/31] Extracting libXtst-1.2.2_3: 100%
          [26/31] Installing java-zoneinfo-2015.g...
          [26/31] Extracting java-zoneinfo-2015.g: 100%
          [27/31] Installing dejavu-2.35...
          [27/31] Extracting dejavu-2.35: 100%
          [28/31] Installing libXrender-0.9.9...
          [28/31] Extracting libXrender-0.9.9: 100%
          [29/31] Installing javavmwrapper-2.5...
          [29/31] Extracting javavmwrapper-2.5: 100%
          [30/31] Installing alsa-lib-1.1.0...
          [30/31] Extracting alsa-lib-1.1.0: 100%
          [31/31] Installing openjdk-7.95.00,1...
          [31/31] Extracting openjdk-7.95.00,1: 100%
          Message from dejavu-2.35:
          Make sure that the freetype module is loaded.  If it is not, add the following
          line to the "Modules" section of your X Windows configuration file:
          
                  Load "freetype"
          
          Add the following line to the "Files" section of X Windows configuration file:
          
                  FontPath "/usr/local/share/fonts/dejavu/"
          
          Note: your X Windows configuration file is typically /etc/X11/XF86Config
          if you are using XFree86, and /etc/X11/xorg.conf if you are using X.Org.
          Message from openjdk-7.95.00,1:
          ======================================================================
          
          This OpenJDK implementation requires fdescfs(5) mounted on /dev/fd and
          procfs(5) mounted on /proc for some functionality.
          
          If you have not done it yet, please do the following:
          
                  mount -t fdescfs fdesc /dev/fd
                  mount -t procfs proc /proc
          
          To make it permanent, you need the following lines in /etc/fstab:
          
                  fdesc   /dev/fd         fdescfs         rw      0       0
                  proc    /proc           procfs          rw      0       0
          
          ======================================================================



 2. Install Hadoop
    unix> pkg search hadoop
          hadoop-1.2.1_3                 Apache Map/Reduce framework
          hadoop2-2.7.2                  Apache Map/Reduce framework

    unix> pkg install hadoop2-2.7.2 
          Proceed with this action? [y/N]:  y  <enter>
         

        HardenedBSD repository is up-to-date.
        All repositories are up-to-date.
        The following 4 package(s) will be affected (of 0 checked):
        
        New packages to be INSTALLED:
                hadoop2: 2.7.2
                ssid: 0.1
                bash: 4.3.42_1
                snappy: 1.1.3
        
        The process will require 226 MiB more space.
        168 MiB to be downloaded.
        
        Proceed with this action? [y/N]: y
        Fetching hadoop2-2.7.2.txz: 100%  166 MiB   1.1MB/s    02:40
        Fetching ssid-0.1.txz: 100%    3 KiB   2.9kB/s    00:01
        Fetching bash-4.3.42_1.txz: 100%    1 MiB 651.1kB/s    00:02
        Fetching snappy-1.1.3.txz: 100%   61 KiB  62.8kB/s    00:01
        Checking integrity... done (0 conflicting)
        [1/4] Installing ssid-0.1...
        [1/4] Extracting ssid-0.1: 100%
        [2/4] Installing bash-4.3.42_1...
        [2/4] Extracting bash-4.3.42_1: 100%
        [3/4] Installing snappy-1.1.3...
        [3/4] Extracting snappy-1.1.3: 100%
        [4/4] Installing hadoop2-2.7.2...
        ===> Creating users and/or groups.
        Creating group 'hadoop' with gid '955'.
        Creating user 'hdfs' with uid '955'.
        Creating user 'mapred' with uid '947'.
        [4/4] Extracting hadoop2-2.7.2: 100%
        Message from bash-4.3.42_1:
        ======================================================================
        
        bash requires fdescfs(5) mounted on /dev/fd
        
        If you have not done it yet, please do the following:
        
                mount -t fdescfs fdesc /dev/fd
        
        To make it permanent, you need the following lines in /etc/fstab:
        
                fdesc   /dev/fd         fdescfs         rw      0       0
        
        ======================================================================



 3. Edit configuration for SECADM to allow java and other stuff
    unix> 



 4. Verify that java can be executed
    unix> java -version
          openjdk version "1.7.0_95"
          OpenJDK Runtime Environment (build 1.7.0_95-b00)
          OpenJDK 64-Bit Server VM (build 24.95-b01, mixed mode)



 5. Configure Hadoop to run in pseudo-distributed mode
    NOTE:  There can be no leading spaces bofre <?xml....>
   
    a. Create this file:  core-site.xml
       unix> cd /usr/local/etc/hadoop
       unix> vi core-site.xml
      
        <?xml version="1.0" encoding="UTF-8"?>
        <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>        
            <configuration>
                <property>
                    <name>fs.defaultFS</name>
                    <value>hdfs://localhost:9000</value>
                </property>
                <property>
                    <name>hadoop.tmp.dir</name>
                    <value>/tank/hdfs/tmp</value>
                </property>
            </configuration>
   
     
    b. Create this file:  hdfs-site.xml
       Note:  Create namenode and datanode directory under /tank/hdfs
       Note:  Set the datanode to listen on port 50001
      
       unix> cd /usr/local/etc/hadoop
       unix> vi hdfs-site.xml
      
           
       <?xml version="1.0" encoding="UTF-8"?>
       <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
            <configuration>
                <property>
                    <name>dfs.replication</name>
                    <value>1</value>
                </property>
                <property>
                    <name>dfs.namenode.name.dir</name>
                    <value>/tank/hdfs/namenode</value>
                </property>
                <property>
                    <name>dfs.datanode.data.dir</name>
                    <value>/tank/hdfs/datanode</value>
                </property>

                <property>
                    <name>dfs.datanode.address</name>
                    <value>localhost:50001</value>
                </property>
                
            </configuration>
      
      
    c. Create this file:  yarn-site.xml
       NOTE:  yarn.nodemanager.resource.memory-mb=100 GB or 102400 MB
       NOTE:  yarn.nodemanager.resource.cpu-vcores=35  (to use 35 or the 40 cores)
       NOTE:  yarn.scheduler.minimum-allocation-mb=2925 so that Hadoop only allocates 35 vcores (102400/2925 = 35)
       
       unix> cd /usr/local/etc/hadoop
       unix> vi yarn-site.xml

        <?xml version="1.0"?>
         <configuration>
            <property>
               <name>yarn.nodemanager.aux-services</name>
               <value>mapreduce_shuffle</value>
            </property>
            <property>
               <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
               <value>org.apache.hadoop.mapred.ShuffleHandler</value>
            </property>
           <property>
               <name>yarn.nodemanager.resource.memory-mb</name>
               <value>102400</value>
            </property>
            <property>
               <name>yarn.scheduler.minimum-allocation-mb</name>
               <value>2925</value>
            </property>
            <property>
               <name>yarn.scheduler.maximum-allocation-mb</name>
               <value>102400</value>
            </property>
            <property>
               <name>yarn.nodemanager.resource.cpu-vcores</name>
               <value>35</value>
            </property>
            <property>
               <name>yarn.scheduler.minimum-allocation-vcores</name>
               <value>1</value>
            </property>
            <property>
               <name>yarn.scheduler.increment-allocation-vcores</name>
               <value>1</value>
            </property>
            <property>
               <name>yarn.scheduler.maximum-allocation-vcores</name>
               <value>35</value>
            </property>
            <property>
               <name>yarn.application.classpath</name>
               <value>
                    $HADOOP_HOME/etc/hadoop,
                    $HADOOP_HOME/share/hadoop/common/*,
                    $HADOOP_HOME/share/hadoop/common/lib/*,
                    $HADOOP_HOME/share/hadoop/mapreduce/*,
                    $HADOOP_HOME/share/hadoop/mapreduce/lib/*,
                    $HADOOP_HOME/share/hadoop/hdfs/*,
                    $HADOOP_HOME/share/hadoop/hdfs/lib/*,          
                    $HADOOP_HOME/share/hadoop/yarn/*,
                    $HADOOP_HOME/share/hadoop/yarn/lib/*,
                    /usr/local/share/hadoop/common/*,
                    /usr/local/share/hadoop/common/lib/*,
                    /usr/local/share/hadoop/mapreduce/*,
                    /usr/local/share/hadoop/mapreduce/lib/*,
                    /usr/local/share/hadoop/hdfs/*,
                    /usr/local/share/hadoop/hdfs/lib/*,
                    /usr/local/share/hadoop/yarn/*,
                    /usr/local/share/hadoop/yarn/lib/*
               </value>
            </property>
        </configuration> 
                 

    d. Create this file:  mapred-site.xml
       unix> cd /usr/local/etc/hadoop
       unix> vi mapred-site.xml
       
             <?xml version="1.0"?>
             <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
             
             <configuration>
                <property>
                   <name>mapreduce.framework.name</name>
                   <value>yarn</value>
                </property>
                <property>
                   <name>yarn.app.mapreduce.am.staging-dir</name>
                   <value>/tank/hdfs/yarn-staging</value>
                </property>
             </configuration>    
    
    
    e. Create this file:  slaves
       unix> cd /usr/local/etc/hadoop
       unix> echo localhost > slaves
       
       
    f. Create this file:  capacity-scheduler.xml
       unix> cd /usr/local/etc/hadoop
       unix> vi capacity-scheduler.xml
       

     <configuration>
     
       <property>
         <name>yarn.scheduler.capacity.maximum-applications</name>
         <value>10000</value>
         <description>
           Maximum number of applications that can be pending and running.
         </description>
       </property>
     
       <property>
         <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
         <value>0.1</value>
         <description>
           Maximum percent of resources in the cluster which can be used to run 
           application masters i.e. controls number of concurrent running
           applications.
         </description>
       </property>
     
       <property>
         <name>yarn.scheduler.capacity.resource-calculator</name>
         <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>
         <description>
           The ResourceCalculator implementation to be used to compare 
           Resources in the scheduler.
           The default i.e. DefaultResourceCalculator only uses Memory while
           DominantResourceCalculator uses dominant-resource to compare 
           multi-dimensional resources such as Memory, CPU etc.
         </description>
       </property>
     
       <property>
         <name>yarn.scheduler.capacity.root.queues</name>
         <value>default</value>
         <description>
           The queues at the this level (root is the root queue).
         </description>
       </property>
     
       <property>
         <name>yarn.scheduler.capacity.root.default.capacity</name>
         <value>100</value>
         <description>Default queue target capacity.</description>
       </property>
     
       <property>
         <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>
         <value>1</value>
         <description>
           Default queue user limit a percentage from 0.0 to 1.0.
         </description>
       </property>
     
       <property>
         <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
         <value>100</value>
         <description>
           The maximum capacity of the default queue. 
         </description>
       </property>
     
       <property>
         <name>yarn.scheduler.capacity.root.default.state</name>
         <value>RUNNING</value>
         <description>
           The state of the default queue. State can be one of RUNNING or STOPPED.
         </description>
       </property>
     
       <property>
         <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>
         <value>*</value>
         <description>
           The ACL of who can submit jobs to the default queue.
         </description>
       </property>
     
       <property>
         <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>
         <value>*</value>
         <description>
           The ACL of who can administer jobs on the default queue.
         </description>
       </property>
     
       <property>
         <name>yarn.scheduler.capacity.node-locality-delay</name>
         <value>40</value>
         <description>
           Number of missed scheduling opportunities after which the CapacityScheduler 
           attempts to schedule rack-local containers. 
           Typically this should be set to number of nodes in the cluster, By default is setting 
           approximately number of nodes in one rack which is 40.
         </description>
       </property>
     
       <property>
         <name>yarn.scheduler.capacity.queue-mappings</name>
         <value></value>
         <description>
           A list of mappings that will be used to assign jobs to queues
           The syntax for this list is [u|g]:[name]:[queue_name][,next mapping]*
           Typically this list will be used to map users to queues,
           for example, u:%user:%user maps all users to queues with the same name
           as the user.
         </description>
       </property>
     
       <property>
         <name>yarn.scheduler.capacity.queue-mappings-override.enable</name>
         <value>false</value>
         <description>
           If a queue mapping is present, will it override the value specified
           by the user? This can be used by administrators to place jobs in queues
           that are different than the one specified by the user.
           The default is false.
         </description>
       </property>
     
     </configuration>



 6. Adjust the hdfs account so it can ssh to localhost [without prompting for password]
    a. Adjust the hdfs account so it has a real home directory
       unix> mkdir /home/hdfs
       unix> chown -R hdfs:hadoop /home/hdfs
       unix> pw user mod hdfs -d /home/hdfs -s /bin/sh
   
    b. Setup the hdfs account so it can ssh to localhost [without prompting for password
       unix> su - hdfs
   
       unix> ssh-keygen -t rsa
             Enter file in which to save the key (/home/hdfs/.ssh/id_rsa):   <press enter>
             
             Enter passphrase (empty for no passphrase):    <press enter>
             Enter same passphrase again:                   <press enter>
 
       unix> cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
       unix> chmod 0600 ~/.ssh/authorized_keys
 
    c. Verify your key based login. 
       NOTE:  This ssh command should *not*  ask for password but first time it will prompt for adding RSA to the list of known hosts.
       unix> ssh localhost
       
       The authenticity of host 'localhost (::1)' can't be established.
       RSA key fingerprint is 9b:35:ab:51:bc:6e:92:ff:a5:4f:2d:a9:11:d5:36:ae.
       Are you sure you want to continue connecting (yes/no)?                    yes
     
      unix> exit



 7. Format the Name Node
    unix> rm -rf /tank/hdfs
    unix> mkdir /tank/hdfs
    unix> chown -R hdfs:hadoop /tank/hdfs
    unix> su - hdfs
    unix> hdfs namenode -format

    You should see this:
    
      2016-03-28 17:22:32,389 INFO  [main] namenode.NameNode (LogAdapter.java:info(47)) - STARTUP_MSG:
      /************************************************************
      STARTUP_MSG: Starting NameNode
      STARTUP_MSG:   host = row6-01.labs.g2-inc.net/10.254.30.26
      STARTUP_MSG:   args = [-format]
      STARTUP_MSG:   version = 2.7.2
      STARTUP_MSG:   classpath = /usr/local/etc/hadoop:/usr/local/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/2.jar . . . [long list of JARs]
      STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2016-03-17T04:49Z
      STARTUP_MSG:   java = 1.7.0_95
      ************************************************************/
      2016-03-28 17:22:32,392 INFO  [main] namenode.NameNode (LogAdapter.java:info(47)) - registered UNIX signal handlers for [TERM, HUP, INT]
      2016-03-28 17:22:32,394 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1417)) - createNameNode [-format]
      Formatting using clusterid: CID-8c3a4f0e-171f-46a8-86d8-b1c69607eced
      2016-03-28 17:22:33,275 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(716)) - No KeyProvider found.
      2016-03-28 17:22:33,275 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(726)) - fsLock is fair:true
      2016-03-28 17:22:33,319 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(239)) - dfs.block.invalidate.limit=1000
      2016-03-28 17:22:33,319 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(245)) - dfs.namenode.datanode.registration.ip-hostname-check=true
      2016-03-28 17:22:33,320 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
      2016-03-28 17:22:33,322 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2016 Mar 28 17:22:33
      2016-03-28 17:22:33,323 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
      2016-03-28 17:22:33,323 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
      2016-03-28 17:22:33,325 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 958.5 MB = 19.2 MB
      2016-03-28 17:22:33,325 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
      2016-03-28 17:22:33,342 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(358)) - dfs.block.access.token.enable=false
      2016-03-28 17:22:33,342 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(344)) - defaultReplication         = 1
      2016-03-28 17:22:33,342 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(345)) - maxReplication             = 512
      2016-03-28 17:22:33,344 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(346)) - minReplication             = 1
      2016-03-28 17:22:33,344 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(347)) - maxReplicationStreams      = 2
      2016-03-28 17:22:33,344 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(348)) - replicationRecheckInterval = 3000
      2016-03-28 17:22:33,344 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - encryptDataTransfer        = false
      2016-03-28 17:22:33,344 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxNumBlocksToLog          = 1000
      2016-03-28 17:22:33,350 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(746)) - fsOwner             = hdfs (auth:SIMPLE)
      2016-03-28 17:22:33,350 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(747)) - supergroup          = supergroup
      2016-03-28 17:22:33,350 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(748)) - isPermissionEnabled = true
      2016-03-28 17:22:33,350 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(759)) - HA Enabled: false
      2016-03-28 17:22:33,352 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(796)) - Append Enabled: true
      2016-03-28 17:22:33,393 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
      2016-03-28 17:22:33,393 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
      2016-03-28 17:22:33,393 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 958.5 MB = 9.6 MB
      2016-03-28 17:22:33,393 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^20 = 1048576 entries
      2016-03-28 17:22:33,396 INFO  [main] namenode.FSDirectory (FSDirectory.java:<init>(235)) - ACLs enabled? false
      2016-03-28 17:22:33,396 INFO  [main] namenode.FSDirectory (FSDirectory.java:<init>(239)) - XAttrs enabled? true
      2016-03-28 17:22:33,396 INFO  [main] namenode.FSDirectory (FSDirectory.java:<init>(247)) - Maximum size of an xattr: 16384
      2016-03-28 17:22:33,396 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(298)) - Caching file names occuring more than 10 times
      2016-03-28 17:22:33,403 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
      2016-03-28 17:22:33,403 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
      2016-03-28 17:22:33,403 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 958.5 MB = 2.4 MB
      2016-03-28 17:22:33,403 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
      2016-03-28 17:22:33,405 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5166)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
      2016-03-28 17:22:33,405 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5167)) - dfs.namenode.safemode.min.datanodes = 0
      2016-03-28 17:22:33,405 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5168)) - dfs.namenode.safemode.extension     = 30000
      2016-03-28 17:22:33,408 INFO  [main] metrics.TopMetrics (TopMetrics.java:logConf(65)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
      2016-03-28 17:22:33,408 INFO  [main] metrics.TopMetrics (TopMetrics.java:logConf(67)) - NNTop conf: dfs.namenode.top.num.users = 10
      2016-03-28 17:22:33,408 INFO  [main] metrics.TopMetrics (TopMetrics.java:logConf(69)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
      2016-03-28 17:22:33,409 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(905)) - Retry cache on namenode is enabled
      2016-03-28 17:22:33,409 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(913)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
      2016-03-28 17:22:33,411 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
      2016-03-28 17:22:33,411 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
      2016-03-28 17:22:33,412 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 958.5 MB = 294.5 KB
      2016-03-28 17:22:33,412 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^15 = 32768 entries
      2016-03-28 17:22:33,457 INFO  [main] namenode.FSImage (FSImage.java:format(158)) - Allocated new BlockPoolId: BP-309118338-10.254.30.26-1459200153427
      2016-03-28 17:22:33,484 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /tank/hdfs/namenode has been successfully formatted.
      2016-03-28 17:22:33,583 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
      2016-03-28 17:22:33,585 INFO  [main] util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 0
      2016-03-28 17:22:33,587 INFO  [Thread-1] namenode.NameNode (LogAdapter.java:info(47)) - SHUTDOWN_MSG:
      /************************************************************
      SHUTDOWN_MSG: Shutting down NameNode at row6-01.labs.g2-inc.net/10.254.30.26
      ************************************************************/



 8. Start HDFS (Start the Name Node and Data Node)
    unix> su - hdfs
    unix> start-dfs.sh
    
    Starting namenodes on [localhost]
    Starting namenodes on [localhost]
    localhost: starting namenode, logging to /var/log/hadoop/hadoop-hdfs-namenode-row6-01.out
    localhost: starting datanode, logging to /var/log/hadoop/hadoop-hdfs-datanode-row6-01.out
    Starting secondary namenodes [0.0.0.0]
    0.0.0.0: starting secondarynamenode, logging to /var/log/hadoop/hadoop-hdfs-secondarynamenode-row6-01.out
    The authenticity of host '0.0.0.0 (0.0.0.0)' can't be established.
    ECDSA key fingerprint is SHA256:A2vDupT1A8ZGNEuMRO0lLeYZAIciZIM+R7awySwWDjs.
    No matching host key fingerprint found in DNS.
    Are you sure you want to continue connecting (yes/no)?  yes



 9. Start MapReduce aka YARN (Resource Manager and Node Manager)
    unix> start-yarn.sh

    starting yarn daemons
    starting resourcemanager, logging to /var/log/hadoop/yarn-yarn-resourcemanager-row6-01.out
    localhost: starting nodemanager, logging to /var/log/hadoop/yarn-yarn-nodemanager-row6-01.out



10. Verify it is running
    a. Go to the NodeManager's url:
       http://localhost:8042/
       *OR*
       unix> telnet localhost 8042
       
    b. Go to the Namenode's website
       http://localhost:50070  
       *OR*
       unix> telnet localhost 50070

    c. Verify that you can create a directory within your HDFS
       1) List what is in your HDFS top directory
          unix> hadoop fs -ls /    
                NOTE:  It should display nothing
         
       2) Create a directory called /tmp in HDFS
          unix> hadoop fs -mkdir /tmp
       
       3) List what is in your HDFS top directory
          unix> hadoop fs -ls /
                Found 1 items
                drwxr-xr-x   - hdfs supergroup          0 2016-03-28 17:38 /tmp

    d. Verify that you can create a file within your HDFS
       1) Create a file called c:\temp\stuff.txt
          unix> echo "Hello There" > /tmp/stuff.txt
          
       2) Use the hadoop command-line to insert this file into your HDFS
          unix> hadoop fs -put /tmp/stuff.txt /tmp/stuff.txt   
 
          unix> hadoop fs -ls /tmp
                Found 1 items
                -rw-r--r--   1 hdfs supergroup         12 2016-03-28 17:41 /tmp/stuff.txt
 
       3) Use the hadoop command-line to pull the HDFS file and copt to your local file system
          unix> hadoop fs -get /tmp/stuff.txt /tmp/stuff2.txt
          
          NOTE:  The /tmp/stuff.txt and /tmp/stuff2.txt files should be identical
          unix> diff /tmp/stuff.txt /tmp/stuff2.txt

       4) Display the content's of a file located in HDFS
          unix> hadoop fs -cat /tmp/stuff.txt


       
Benchmark your HDFS with teragen
--------------------------------
 1. Run the teragen, terasort, and teraverify map-reduce jnobs
 
    a. Generate the TeraSort input data 
       unix> cd /usr/local/share/hadoop/mapreduce/
       unix> hadoop jar hadoop-mapreduce-examples-2.7.2.jar teragen 10000000000  /terasort-input
      
             W A I T     9 0  +    M I N U T E S 
             
             Feel free to look at the job status using this url:
                http://serverHostName:8088/cluster/apps/RUNNING
                
  
        2016-03-28 20:16:54,187 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 100% reduce 0%
        2016-03-28 20:17:36,851 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1378)) - Job job_1459204659843_0005 completed successfull y
        2016-03-28 20:17:36,977 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1385)) - Counters: 31
                File System Counters
                        FILE: Number of bytes read=0
                        FILE: Number of bytes written=236808
                        FILE: Number of read operations=0
                        FILE: Number of large read operations=0
                        FILE: Number of write operations=0
                        HDFS: Number of bytes read=173
                        HDFS: Number of bytes written=1000000000000
                        HDFS: Number of read operations=8
                        HDFS: Number of large read operations=0
                        HDFS: Number of write operations=4
                Job Counters
                        Launched map tasks=2
                        Other local map tasks=2
                        Total time spent by all maps in occupied slots (ms)=10543513
                        Total time spent by all reduces in occupied slots (ms)=0
                        Total time spent by all map tasks (ms)=10543513
                        Total vcore-milliseconds taken by all map tasks=10543513
                        Total megabyte-milliseconds taken by all map tasks=10796557312
                Map-Reduce Framework
                        Map input records=10000000000
                        Map output records=10000000000
                        Input split bytes=173
                        Spilled Records=0
                        Failed Shuffles=0
                        Merged Map outputs=0
                        GC time elapsed (ms)=153100
                        CPU time spent (ms)=0
                        Physical memory (bytes) snapshot=0
                        Virtual memory (bytes) snapshot=0
                        Total committed heap usage (bytes)=305135616
                org.apache.hadoop.examples.terasort.TeraGen$Counters
                        CHECKSUM=3028416809717741100
                File Input Format Counters
                        Bytes Read=0
                File Output Format Counters
                        Bytes Written=1000000000000

  
             
       NOTE:  The parameter supplied to TeraGen is 10 billion (10,000,000,000) and not 1 trillion.
              The reason is that the first parameter specifies the number of rows of input data to generate
               -- Each input data has a size of 100 bytes
               -- 10 billion * 100 = 1 trillion bytes.
      
       unix> hadoop fs -ls /
             -- You should see a /terasort-input directory

  
    b. Sort the input data
       unix> cd /usr/local/share/hadoop/mapreduce/
       unix> hadoop jar hadoop-mapreduce-examples-2.7.2.jar terasort   /terasort-input  /terasort-output

             W A I T     6 +    H O U R S 

       unix> hadoop fs -ls /
             -- You should see a /terasort-input directory
             -- You should see a /terasort-output directory


    c. Validate that the data was actually sorted
       unix> cd /usr/local/share/hadoop/mapreduce/
       unix> hadoop jar hadoop-mapreduce-examples-2.7.2.jar teravalidate    /terasort-output   /terasort-validate


    d. Look at the statistics for this job
       unix> hadoop job -history all /terasort-output


    e. **OPTIONAL** Cleanup:  Delete the created HDFS directories
       unix> hadoop fs -rm -r -f /terasort-input /terasort-output /terasort-validate



NOTE:  If you kill your sort job, then you must delete the /terasort-output directory before restarting
--------------------------------------------------------------------------------------------------------
unix> hadoop fs -rm -r -f /terasort-output    # terasort-output
unix> cd /usr/local/share/hadoop/mapreduce/
unix> hadoop jar hadoop-mapreduce-examples-2.7.2.jar terasort   /terasort-input  /terasort-output   # Restart job



How to Kill a Hadoop Job via Command-Line
-----------------------------------------
 unix> yarn application -list
       -- Look for the applicationid
       
 unix> yarn application -kill <application-Id>
 
