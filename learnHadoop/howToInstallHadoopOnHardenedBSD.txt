How to Install Hadoop on Hardened BSD
-------------------------------------

Assumptions:
 A) You are root
 
 
Procedures
----------
 1. Install Java 7 JDK
    unix> sudo -i    # change over as root
    
    unix> pkg search "^openjdk"   
          openjdk-7.95.00,1              Java Development Kit 7
          openjdk-jre-7.95.00,1          Java Runtime Environment 7
          openjdk6-b38,1                 Oracle's Java 6 virtual machine release under the GPL v2
          openjdk6-jre-b38,1             Oracle's Java 6 Runtime Environment under the GPL v2
          openjdk8-8.72.15               Java Development Kit 8
          openjdk8-jre-8.72.15           Java Runtime Environment 8

    unix> pkg install openjdk-7.95.00,1
          Proceed with this action? [y/N]:  y  <enter>
          
          [1/31] Installing xproto-7.0.28...
          [1/31] Extracting xproto-7.0.28: 100%
          [2/31] Installing libXdmcp-1.1.2...
          [2/31] Extracting libXdmcp-1.1.2: 100%
          [3/31] Installing libxml2-2.9.3...
          [3/31] Extracting libxml2-2.9.3: 100%
          [4/31] Installing libpthread-stubs-0.3_6...
          [4/31] Extracting libpthread-stubs-0.3_6: 100%
          [5/31] Installing libXau-1.0.8_3...
          [5/31] Extracting libXau-1.0.8_3: 100%
          [6/31] Installing libxcb-1.11.1...
          [6/31] Extracting libxcb-1.11.1: 100%
          [7/31] Installing kbproto-1.0.7...
          [7/31] Extracting kbproto-1.0.7: 100%
          [8/31] Installing libX11-1.6.3,1...
          [8/31] Extracting libX11-1.6.3,1: 100%
          [9/31] Installing xextproto-7.3.0...
          [9/31] Extracting xextproto-7.3.0: 100%
          [10/31] Installing fixesproto-5.0...
          [10/31] Extracting fixesproto-5.0: 100%
          [11/31] Installing freetype2-2.6.3...
          [11/31] Extracting freetype2-2.6.3: 100%
          [12/31] Installing libfontenc-1.1.3...
          [12/31] Extracting libfontenc-1.1.3: 100%
          [13/31] Installing libICE-1.0.9_1,1...
          [13/31] Extracting libICE-1.0.9_1,1: 100%
          [14/31] Installing inputproto-2.3.1...
          [14/31] Extracting inputproto-2.3.1: 100%
          [15/31] Installing libXext-1.3.3_1,1...
          [15/31] Extracting libXext-1.3.3_1,1: 100%
          [16/31] Installing libXfixes-5.0.1_3...
          [16/31] Extracting libXfixes-5.0.1_3: 100%
          [17/31] Installing mkfontscale-1.1.2...
          [17/31] Extracting mkfontscale-1.1.2: 100%
          [18/31] Installing libSM-1.2.2_3,1...
          [18/31] Extracting libSM-1.2.2_3,1: 100%
          [19/31] Installing recordproto-1.14.2...
          [19/31] Extracting recordproto-1.14.2: 100%
          [20/31] Installing libXi-1.7.6,1...
          [20/31] Extracting libXi-1.7.6,1: 100%
          [21/31] Installing fontconfig-2.11.1_1,1...
          [21/31] Extracting fontconfig-2.11.1_1,1: 100%
          Running fc-cache to build fontconfig cache...
          /usr/local/share/fonts: skipping, no such directory
          /usr/local/lib/X11/fonts: skipping, no such directory
          /root/.local/share/fonts: skipping, no such directory
          /root/.fonts: skipping, no such directory
          /var/db/fontconfig: cleaning cache directory
          /root/.cache/fontconfig: not cleaning non-existent cache directory
          /root/.fontconfig: not cleaning non-existent cache directory
          fc-cache: succeeded
          [22/31] Installing mkfontdir-1.0.7...
          [22/31] Extracting mkfontdir-1.0.7: 100%
          [23/31] Installing renderproto-0.11.1...
          [23/31] Extracting renderproto-0.11.1: 100%
          [24/31] Installing libXt-1.1.5,1...
          [24/31] Extracting libXt-1.1.5,1: 100%
          [25/31] Installing libXtst-1.2.2_3...
          [25/31] Extracting libXtst-1.2.2_3: 100%
          [26/31] Installing java-zoneinfo-2015.g...
          [26/31] Extracting java-zoneinfo-2015.g: 100%
          [27/31] Installing dejavu-2.35...
          [27/31] Extracting dejavu-2.35: 100%
          [28/31] Installing libXrender-0.9.9...
          [28/31] Extracting libXrender-0.9.9: 100%
          [29/31] Installing javavmwrapper-2.5...
          [29/31] Extracting javavmwrapper-2.5: 100%
          [30/31] Installing alsa-lib-1.1.0...
          [30/31] Extracting alsa-lib-1.1.0: 100%
          [31/31] Installing openjdk-7.95.00,1...
          [31/31] Extracting openjdk-7.95.00,1: 100%
          Message from dejavu-2.35:
          Make sure that the freetype module is loaded.  If it is not, add the following
          line to the "Modules" section of your X Windows configuration file:
          
                  Load "freetype"
          
          Add the following line to the "Files" section of X Windows configuration file:
          
                  FontPath "/usr/local/share/fonts/dejavu/"
          
          Note: your X Windows configuration file is typically /etc/X11/XF86Config
          if you are using XFree86, and /etc/X11/xorg.conf if you are using X.Org.
          Message from openjdk-7.95.00,1:
          ======================================================================
          
          This OpenJDK implementation requires fdescfs(5) mounted on /dev/fd and
          procfs(5) mounted on /proc for some functionality.
          
          If you have not done it yet, please do the following:
          
                  mount -t fdescfs fdesc /dev/fd
                  mount -t procfs proc /proc
          
          To make it permanent, you need the following lines in /etc/fstab:
          
                  fdesc   /dev/fd         fdescfs         rw      0       0
                  proc    /proc           procfs          rw      0       0
          
          ======================================================================
          
    
 2. Install Hadoop
    unix> pkg search hadoop
          hadoop-1.2.1_3                 Apache Map/Reduce framework
          hadoop2-2.7.2                  Apache Map/Reduce framework

    unix> pkg install hadoop2-2.7.2 
          Proceed with this action? [y/N]:  y  <enter>
         

        HardenedBSD repository is up-to-date.
        All repositories are up-to-date.
        The following 4 package(s) will be affected (of 0 checked):
        
        New packages to be INSTALLED:
                hadoop2: 2.7.2
                ssid: 0.1
                bash: 4.3.42_1
                snappy: 1.1.3
        
        The process will require 226 MiB more space.
        168 MiB to be downloaded.
        
        Proceed with this action? [y/N]: y
        Fetching hadoop2-2.7.2.txz: 100%  166 MiB   1.1MB/s    02:40
        Fetching ssid-0.1.txz: 100%    3 KiB   2.9kB/s    00:01
        Fetching bash-4.3.42_1.txz: 100%    1 MiB 651.1kB/s    00:02
        Fetching snappy-1.1.3.txz: 100%   61 KiB  62.8kB/s    00:01
        Checking integrity... done (0 conflicting)
        [1/4] Installing ssid-0.1...
        [1/4] Extracting ssid-0.1: 100%
        [2/4] Installing bash-4.3.42_1...
        [2/4] Extracting bash-4.3.42_1: 100%
        [3/4] Installing snappy-1.1.3...
        [3/4] Extracting snappy-1.1.3: 100%
        [4/4] Installing hadoop2-2.7.2...
        ===> Creating users and/or groups.
        Creating group 'hadoop' with gid '955'.
        Creating user 'hdfs' with uid '955'.
        Creating user 'mapred' with uid '947'.
        [4/4] Extracting hadoop2-2.7.2: 100%
        Message from bash-4.3.42_1:
        ======================================================================
        
        bash requires fdescfs(5) mounted on /dev/fd
        
        If you have not done it yet, please do the following:
        
                mount -t fdescfs fdesc /dev/fd
        
        To make it permanent, you need the following lines in /etc/fstab:
        
                fdesc   /dev/fd         fdescfs         rw      0       0
        
        ======================================================================

 3. Edit configuration for SECADM to allow java and other stuff
    unix> 
   
   
   
 4. Verify that java can be executed
    unix> java -version
          openjdk version "1.7.0_95"
          OpenJDK Runtime Environment (build 1.7.0_95-b00)
          OpenJDK 64-Bit Server VM (build 24.95-b01, mixed mode)

   
 5. Configure Hadoop to run in pseudo-distributed mode
    NOTE:  There can be no leading spaces bofre <?xml....>
   
    a. Create the core-site.xml
      unix> cd /usr/local/etc/hadoop
      unix> vi core-site.xml
      
        <?xml version="1.0" encoding="UTF-8"?>
        <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>        
            <configuration>
                <property>
                    <name>fs.defaultFS</name>
                    <value>hdfs://localhost:9000</value>
                </property>
            </configuration>
   
   
     
    b. Create the hdfs-site.xml
       Note:  Create namenode and datanode directory under /tank/hdfs
       Note:  Set the datanode to listen on port 50001
      
       unix> cd /usr/local/etc/hadoop
       unix> vi hdfs-site.xml
      
           
       <?xml version="1.0" encoding="UTF-8"?>
       <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
            <configuration>
                <property>
                    <name>dfs.replication</name>
                    <value>1</value>
                </property>
                <property>
                    <name>dfs.namenode.name.dir</name>
                    <value>file:/tank/hdfs/namenode</value>
                </property>
                <property>
                    <name>dfs.datanode.data.dir</name>
                    <value>file:/tank/hdfs/datanode</value>
                </property>

                <property>
                    <name>dfs.datanode.address</name>
                    <value>localhost:50001</value>
                </property>
            </configuration>
      
      
    c. Create the yarn-site.xml
       unix> cd /usr/local/etc/hadoop
       unix> vi yarn-site.xml

        <?xml version="1.0"?>
         <configuration>
            <property>
               <name>yarn.nodemanager.aux-services</name>
               <value>mapreduce_shuffle</value>
            </property>
            <property>
               <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
               <value>org.apache.hadoop.mapred.ShuffleHandler</value>
            </property>
            <property>
               <name>yarn.application.classpath</name>
               <value>
                    $HADOOP_HOME/etc/hadoop,
                    $HADOOP_HOME/share/hadoop/common/*,
                    $HADOOP_HOME/share/hadoop/common/lib/*,
                    $HADOOP_HOME/share/hadoop/mapreduce/*,
                    $HADOOP_HOME/share/hadoop/mapreduce/lib/*,
                    $HADOOP_HOME/share/hadoop/hdfs/*,
                    $HADOOP_HOME/share/hadoop/hdfs/lib/*,          
                    $HADOOP_HOME/share/hadoop/yarn/*,
                    $HADOOP_HOME/share/hadoop/yarn/lib/*
               </value>
            </property>
        </configuration> 
                 

    d. Create the mapred-site.xml
       unix> cd /usr/local/etc/hadoop
       unix> vi mapred-site.xml
       
             <?xml version="1.0"?>
             <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
             
             <configuration>
                <property>
                   <name>mapreduce.framework.name</name>
                   <value>yarn</value>
                </property>
             </configuration>    
    
    
    e. Create a slaces file
       unix> cd /usr/local/etc/hadoop
       unix> echo localhost > slaves
       
             

 6. Adjust the hdfs account so it can ssh to localhost [without prompting for password]
    a. Adjust the hdfs account so it has a real home directory
       unix> mkdir /home/hdfs
       unix> chown -R hdfs:hadoop /home/hdfs
       unix> pw user mod hdfs -d /home/hdfs -s /bin/sh
   
    b. Setup the hdfs account so it can ssh to localhost [without prompting for password
       unix> su - hdfs
   
       unix> ssh-keygen -t rsa
             Enter file in which to save the key (/home/hdfs/.ssh/id_rsa):   <press enter>
             
             Enter passphrase (empty for no passphrase):    <press enter>
             Enter same passphrase again:                   <press enter>
 
       unix> cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
       unix> chmod 0600 ~/.ssh/authorized_keys
 
    c. Verify your key based login. 
       NOTE:  This ssh command should *not*  ask for password but first time it will prompt for adding RSA to the list of known hosts.
       unix> ssh localhost
       
       The authenticity of host 'localhost (::1)' can't be established.
       RSA key fingerprint is 9b:35:ab:51:bc:6e:92:ff:a5:4f:2d:a9:11:d5:36:ae.
       Are you sure you want to continue connecting (yes/no)?                    yes
     
      unix> exit


 7. Format the Name Node
    unix> mkdir /tank/hdfs
    unix> chown -R hdfs:hadoop /tank 
    unix> su - hdfs
    unix> hdfs namenode -format

    You should see this:
    
      2016-03-28 17:22:32,389 INFO  [main] namenode.NameNode (LogAdapter.java:info(47)) - STARTUP_MSG:
      /************************************************************
      STARTUP_MSG: Starting NameNode
      STARTUP_MSG:   host = row6-01.labs.g2-inc.net/10.254.30.26
      STARTUP_MSG:   args = [-format]
      STARTUP_MSG:   version = 2.7.2
      STARTUP_MSG:   classpath = /usr/local/etc/hadoop:/usr/local/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/2.jar . . . [long list of JARs]
      STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2016-03-17T04:49Z
      STARTUP_MSG:   java = 1.7.0_95
      ************************************************************/
      2016-03-28 17:22:32,392 INFO  [main] namenode.NameNode (LogAdapter.java:info(47)) - registered UNIX signal handlers for [TERM, HUP, INT]
      2016-03-28 17:22:32,394 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1417)) - createNameNode [-format]
      Formatting using clusterid: CID-8c3a4f0e-171f-46a8-86d8-b1c69607eced
      2016-03-28 17:22:33,275 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(716)) - No KeyProvider found.
      2016-03-28 17:22:33,275 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(726)) - fsLock is fair:true
      2016-03-28 17:22:33,319 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(239)) - dfs.block.invalidate.limit=1000
      2016-03-28 17:22:33,319 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(245)) - dfs.namenode.datanode.registration.ip-hostname-check=true
      2016-03-28 17:22:33,320 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
      2016-03-28 17:22:33,322 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2016 Mar 28 17:22:33
      2016-03-28 17:22:33,323 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
      2016-03-28 17:22:33,323 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
      2016-03-28 17:22:33,325 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 958.5 MB = 19.2 MB
      2016-03-28 17:22:33,325 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
      2016-03-28 17:22:33,342 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(358)) - dfs.block.access.token.enable=false
      2016-03-28 17:22:33,342 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(344)) - defaultReplication         = 1
      2016-03-28 17:22:33,342 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(345)) - maxReplication             = 512
      2016-03-28 17:22:33,344 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(346)) - minReplication             = 1
      2016-03-28 17:22:33,344 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(347)) - maxReplicationStreams      = 2
      2016-03-28 17:22:33,344 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(348)) - replicationRecheckInterval = 3000
      2016-03-28 17:22:33,344 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - encryptDataTransfer        = false
      2016-03-28 17:22:33,344 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxNumBlocksToLog          = 1000
      2016-03-28 17:22:33,350 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(746)) - fsOwner             = hdfs (auth:SIMPLE)
      2016-03-28 17:22:33,350 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(747)) - supergroup          = supergroup
      2016-03-28 17:22:33,350 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(748)) - isPermissionEnabled = true
      2016-03-28 17:22:33,350 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(759)) - HA Enabled: false
      2016-03-28 17:22:33,352 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(796)) - Append Enabled: true
      2016-03-28 17:22:33,393 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
      2016-03-28 17:22:33,393 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
      2016-03-28 17:22:33,393 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 958.5 MB = 9.6 MB
      2016-03-28 17:22:33,393 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^20 = 1048576 entries
      2016-03-28 17:22:33,396 INFO  [main] namenode.FSDirectory (FSDirectory.java:<init>(235)) - ACLs enabled? false
      2016-03-28 17:22:33,396 INFO  [main] namenode.FSDirectory (FSDirectory.java:<init>(239)) - XAttrs enabled? true
      2016-03-28 17:22:33,396 INFO  [main] namenode.FSDirectory (FSDirectory.java:<init>(247)) - Maximum size of an xattr: 16384
      2016-03-28 17:22:33,396 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(298)) - Caching file names occuring more than 10 times
      2016-03-28 17:22:33,403 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
      2016-03-28 17:22:33,403 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
      2016-03-28 17:22:33,403 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 958.5 MB = 2.4 MB
      2016-03-28 17:22:33,403 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
      2016-03-28 17:22:33,405 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5166)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
      2016-03-28 17:22:33,405 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5167)) - dfs.namenode.safemode.min.datanodes = 0
      2016-03-28 17:22:33,405 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5168)) - dfs.namenode.safemode.extension     = 30000
      2016-03-28 17:22:33,408 INFO  [main] metrics.TopMetrics (TopMetrics.java:logConf(65)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
      2016-03-28 17:22:33,408 INFO  [main] metrics.TopMetrics (TopMetrics.java:logConf(67)) - NNTop conf: dfs.namenode.top.num.users = 10
      2016-03-28 17:22:33,408 INFO  [main] metrics.TopMetrics (TopMetrics.java:logConf(69)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
      2016-03-28 17:22:33,409 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(905)) - Retry cache on namenode is enabled
      2016-03-28 17:22:33,409 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(913)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
      2016-03-28 17:22:33,411 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
      2016-03-28 17:22:33,411 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
      2016-03-28 17:22:33,412 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 958.5 MB = 294.5 KB
      2016-03-28 17:22:33,412 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^15 = 32768 entries
      2016-03-28 17:22:33,457 INFO  [main] namenode.FSImage (FSImage.java:format(158)) - Allocated new BlockPoolId: BP-309118338-10.254.30.26-1459200153427
      2016-03-28 17:22:33,484 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /tank/hdfs/namenode has been successfully formatted.
      2016-03-28 17:22:33,583 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
      2016-03-28 17:22:33,585 INFO  [main] util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 0
      2016-03-28 17:22:33,587 INFO  [Thread-1] namenode.NameNode (LogAdapter.java:info(47)) - SHUTDOWN_MSG:
      /************************************************************
      SHUTDOWN_MSG: Shutting down NameNode at row6-01.labs.g2-inc.net/10.254.30.26
      ************************************************************/


 8. Start HDFS (Start the Name Node and Data Node)
    unix> su - hdfs
    unix> start-dfs.sh
    
    Starting namenodes on [localhost]
    Starting namenodes on [localhost]
    localhost: starting namenode, logging to /var/log/hadoop/hadoop-hdfs-namenode-row6-01.out
    localhost: starting datanode, logging to /var/log/hadoop/hadoop-hdfs-datanode-row6-01.out
    Starting secondary namenodes [0.0.0.0]
    0.0.0.0: starting secondarynamenode, logging to /var/log/hadoop/hadoop-hdfs-secondarynamenode-row6-01.out
    The authenticity of host '0.0.0.0 (0.0.0.0)' can't be established.
    ECDSA key fingerprint is SHA256:A2vDupT1A8ZGNEuMRO0lLeYZAIciZIM+R7awySwWDjs.
    No matching host key fingerprint found in DNS.
    Are you sure you want to continue connecting (yes/no)?  yes


 9. Start MapReduce aka YARN (Resource Manager and Node Manager)
    unix> start-yarn.sh

    starting yarn daemons
    starting resourcemanager, logging to /var/log/hadoop/yarn-yarn-resourcemanager-row6-01.out
    localhost: starting nodemanager, logging to /var/log/hadoop/yarn-yarn-nodemanager-row6-01.out



10. Verify it is running
    a. Go to the NodeManager's url:
       http://localhost:8042/
       *OR*
       unix> telnet localhost 8042
 
       
    b. Go to the Namenode's website
       http://localhost:50070  
       *OR*
       unix> telnet localhost 50070


    c. Verify that you can create a directory within your HDFS
       1) List what is in your HDFS top directory
          unix> hadoop fs -ls /    
                NOTE:  It should display nothing
         
       2) Create a directory called /tmp in HDFS
          unix> hadoop fs -mkdir /tmp
       
       3) List what is in your HDFS top directory
          unix> hadoop fs -ls /
                Found 1 items
                drwxr-xr-x   - hdfs supergroup          0 2016-03-28 17:38 /tmp


    d. Verify that you can create a file within your HDFS
       1) Create a file called c:\temp\stuff.txt
          unix> echo "Hello There" > /tmp/stuff.txt
          
       2) Use the hadoop command-line to insert this file into your HDFS
          unix> hadoop fs -put /tmp/stuff.txt /tmp/stuff.txt   
 
          unix> hadoop fs -ls /tmp
                Found 1 items
                -rw-r--r--   1 hdfs supergroup         12 2016-03-28 17:41 /tmp/stuff.txt
 
       3) Use the hadoop command-line to pull the HDFS file and copt to your local file system
          unix> hadoop fs -get /tmp/stuff.txt /tmp/stuff2.txt
          
          NOTE:  The /tmp/stuff.txt and /tmp/stuff2.txt files should be identical
          unix> diff /tmp/stuff.txt /tmp/stuff2.txt


       4) Display the content's of a file located in HDFS
          unix> hadoop fs -cat /tmp/stuff.txt
       
