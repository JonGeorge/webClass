How to Install Cloudera 5.3.3 on Centos 6
-----------------------------------------


References
----------
http://www.cloudera.com/documentation/cdh/5-1-x/CDH5-Installation-Guide/cdh5ig_cdh5_install.html


Procedures
----------
 1. Download and install the CDH 5 "1-click Install" package:
    unix> sudo su -
    unix> wget http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86_64/cloudera-cdh-5-0.x86_64.rpm
    
    unix> sudo yum --nogpgcheck localinstall cloudera-cdh-5-0.x86_64.rpm  

        
        ====================================================================================================================================
         Package                         Arch                      Version                Repository                                   Size
        ====================================================================================================================================
        Installing:
         cloudera-cdh                    x86_64                    5-0                    /cloudera-cdh-5-0.x86_64                     13 k
        
        Transaction Summary
        ====================================================================================================================================
        Install       1 Package(s)
        
        Total size: 13 k
        Installed size: 13 k
        Is this ok [y/N]: y
        Downloading Packages:
        Running rpm_check_debug
        Running Transaction Test
        Transaction Test Succeeded
        Running Transaction
          Installing : cloudera-cdh-5-0.x86_64                                                                                          1/1 
          Verifying  : cloudera-cdh-5-0.x86_64                                                                                          1/1 
        
        Installed:
          cloudera-cdh.x86_64 0:5-0                                                                                                         
        
        Complete!


 2. Install YARN
    unix> sudo yum clean all; sudo yum install hadoop-yarn-resourcemanager
  
  
 3. Install namenode
    unix> sudo yum clean all; sudo yum install hadoop-hdfs-namenode


 4. Install secondary-namenode
    unix> sudo yum clean all; sudo yum install hadoop-hdfs-secondarynamenode
    
    Now, you should have these services here:
       /etc/init.d/hadoop-hdfs-namenode
       /etc/init.d/hadoop-hdfs-secondarynamenode
       /etc/init.d/hadoop-mapreduce-historyserver
       /etc/init.d/hadoop-yarn-proxyserver
       /etc/init.d/hadoop-yarn-resourcemanager
       
 5. Install data-node
    unix> sudo yum clean all; sudo yum install hadoop-hdfs-datanode
  
    
 6. Configure Hadoop to run in pseudo-distributed mode
    NOTE:  We are setting hadoop's temp directory
    
    a. Edit this file:  core-site.xml
       unix> cd /etc/hadoop/conf
       unix> vi core-site.xml
      
        <?xml version="1.0" encoding="UTF-8"?>
        <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>        
            <configuration>
                <property>
                    <name>fs.defaultFS</name>
                    <value>hdfs://localhost:9000</value>
                </property>
                <property>
                    <name>hadoop.tmp.dir</name>
                    <value>/tank/hadoop/tmp</value>
                </property>
            </configuration>
   
     
    b. Edit this file:  hdfs-site.xml
       Note:  Create namenode and datanode directory under /tank/hadoop
       Note:  Set the datanode to listen on port 50001
      
       unix> cd /etc/hadoop/config
       unix> cp hdfs-site.xml hdfs-site.xml.ORIG
       unix> vi hdfs-site.xml
      
           
       <?xml version="1.0" encoding="UTF-8"?>
       <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
            <configuration>
                <property>
                    <name>dfs.replication</name>
                    <value>1</value>
                </property>
                <property>
                    <name>dfs.namenode.name.dir</name>
                    <value>/tank/hadoop/namenode</value>
                </property>
                <property>
                    <name>dfs.datanode.data.dir</name>
                    <value>/tank/hadoop/datanode</value>
                </property>

                <property>
                    <name>dfs.datanode.address</name>
                    <value>localhost:50001</value>
                </property>
                
            </configuration>
      
      
    c. Edit this file:  yarn-site.xml
       NOTE:  yarn.nodemanager.resource.memory-mb=4096 MB (or 4 GB) -- that is a ceiling on how much memory hadoop will use
       NOTE:  yarn.nodemanager.resource.cpu-vcores=4  (to use 4 of the total 4 cores)
       NOTE:  yarn.scheduler.minimum-allocation-mb=1024 so that Hadoop only allocates 4 vcores (4096/4 = 1024)
       
       unix> cd /etc/hadoop/conf
       unix> vi yarn-site.xml

        <?xml version="1.0"?>
         <configuration>
            <property>
               <name>yarn.nodemanager.aux-services</name>
               <value>mapreduce_shuffle</value>
            </property>
            <property>
               <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
               <value>org.apache.hadoop.mapred.ShuffleHandler</value>
            </property>
           <property>
               <name>yarn.nodemanager.resource.memory-mb</name>
               <value>4096</value>
            </property>
            <property>
               <name>yarn.scheduler.minimum-allocation-mb</name>
               <value>1024</value>
            </property>
            <property>
               <name>yarn.scheduler.maximum-allocation-mb</name>
               <value>4096</value>
            </property>
            <property>
               <name>yarn.nodemanager.resource.cpu-vcores</name>
               <value>4</value>
            </property>
            <property>
               <name>yarn.scheduler.minimum-allocation-vcores</name>
               <value>1</value>
            </property>
            <property>
               <name>yarn.scheduler.increment-allocation-vcores</name>
               <value>1</value>
            </property>
            <property>
               <name>yarn.scheduler.maximum-allocation-vcores</name>
               <value>4</value>
            </property>
            <property>
               <name>yarn.application.classpath</name>
               <value>
                    $HADOOP_HOME/etc/hadoop,
                    $HADOOP_HOME/share/hadoop/common/*,
                    $HADOOP_HOME/share/hadoop/common/lib/*,
                    $HADOOP_HOME/share/hadoop/mapreduce/*,
                    $HADOOP_HOME/share/hadoop/mapreduce/lib/*,
                    $HADOOP_HOME/share/hadoop/hdfs/*,
                    $HADOOP_HOME/share/hadoop/hdfs/lib/*,          
                    $HADOOP_HOME/share/hadoop/yarn/*,
                    $HADOOP_HOME/share/hadoop/yarn/lib/*
               </value>
            </property>
        </configuration> 
                 

    d. Create this file:  mapred-site.xml
       unix> cd /opt/hadoop-2.7.2/etc/hadoop
       unix> cp mapred-site.xml.template mapred-site.xml
       unix> vi mapred-site.xml
       
             <?xml version="1.0"?>
             <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
             
             <configuration>
                <property>
                   <name>mapreduce.framework.name</name>
                   <value>yarn</value>
                </property>
                <property>
                   <name>yarn.app.mapreduce.am.staging-dir</name>
                   <value>/user</value>
                   <description>The staging dir (inside HDFS) that is  used while submitting jobs.</description>
                </property>
             </configuration>    
    
    
    e. Create this file:  slaves
       unix> cd /opt/hadoop-2.7.2/etc/hadoop
       unix> echo localhost > slaves
       
7. Format the Name Node
   NOTE:  Cloudera services run under the user alled 'hdfs'
    unix> su -
    unix> rm -rf /tank/hadoop
    unix> mkdir -p /tank/hadoop
    unix> chown -R hdfs:hdfs /tank/hadoop
    
    unix> su - hdfs
    unix> hdfs namenode -format

 8. Start the data node
    unix> sudo service hadoop-hdfs-datanode start
          
    unix> tail -f /var/log/hadoop-hdfs/hadoop-hdfs-datanode-centosVm.log
   
          
 9. Start the name node
    unix> sudo service hadoop-hdfs-namenode start
    
    unix> tail -f /var/log/hadoop-hdfs/hadoop-hdfs-namenode-centosVm.log
    
10. Start the secondary name node
    unix> service hadoop-hdfs-secondarynamenode start

11. Start the YARN resource manager
    unix> service hadoop-yarn-resourcemanager start
    
    unix> tail -f /var/log/hadoop-yarn/yarn-yarn-resourcemanager-centosVm.out
    
    
12. Start the History server [that listens on port 19888]
    unix> service hadoop-mapreduce-historyserver start
    PROBLEM does not work
    


13. Verify it is running
    a. Go to the Namenode's website
       http://localhost:50070  
       *OR*
       unix> telnet localhost 50070   # you should be connected

    b. Go to the Hadoop-Application site
       http://localhost:8088
       *OR*
       unix> telnet localhost 8088    # you should be connected
       
    d. Verify that you can create a directory within your HDFS
       NOTE:  Run these commands as the hdfs unix user
       
       1) List what is in your HDFS top directory
          unix> su - hdfs
          unix> hadoop fs -ls /    
                NOTE:  It should display nothing
         
       2) Create a directory called /tmp in HDFS
          unix> hadoop fs -mkdir /tmp
       
       3) List what is in your HDFS top directory
          unix> hadoop fs -ls /
                Found 1 items
                drwxr-xr-x   - hdfs supergroup          0 2016-03-28 17:38 /tmp

    e. Verify that you can create a file within your HDFS
       1) Create a file called c:\temp\stuff.txt
          unix> echo "Hello There" > /tmp/stuff.txt
          
       2) Use the hadoop command-line to insert this file into your HDFS
          unix> hadoop fs -put /tmp/stuff.txt /tmp/stuff.txt   
 
          unix> hadoop fs -ls /tmp
                Found 1 items
                -rw-r--r--   1 hdfs supergroup         12 2016-03-28 17:41 /tmp/stuff.txt
 
       3) Use the hadoop command-line to pull the HDFS file and copt to your local file system
          unix> hadoop fs -get /tmp/stuff.txt /tmp/stuff2.txt
          
          NOTE:  The /tmp/stuff.txt and /tmp/stuff2.txt files should be identical
          unix> diff /tmp/stuff.txt /tmp/stuff2.txt

       4) Display the content's of a file located in HDFS
          unix> hadoop fs -cat /tmp/stuff.txt

       5) Delete the /tmp directory located in the HDFS
          unix> hadoop fs -rm -r -f /tmp


       

