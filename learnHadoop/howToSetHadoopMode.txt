How to Set the Hadoop "Mode"
----------------------------

Hadoop runs in one of 3 modes:
  A) Local mode 
      + You can debug the job main class
      + You can debug the mapper and reducer code (because the mapper and reducer run in a single JVM)
      + There are no daemons, everything runs on a single JVM
      + It has no DFS and utilizes the local file system
      
  B) Psuedo-distributed mode
      + You can debug the hadoop job main class
      + All daemons run on the same machine
      + You cannot debug the mapper and reducer code (it runs in a different JVM than the driver)
      
          $HADOOP_HOME/conf/core-site.xml: 
      
  C) Cluster mode (Fully distributed mode)
      + Production mode
      + You cannot debug anything
      + Hadoop daemons run on a cluster of machines. 
      + There is one host onto which Namenode is running and another host on which datanode is running
      + There are machines on which task tracker is running. 
      + We have separate masters and separate slaves in this distribution.


NOTE:  In Hadoop 1.0, use fs.default.name
       In Hadoop 2.0, use fs.defaultFS


How to set Hadoop to run in Local mode
--------------------------------------
1. Edit the $HADOOP_HOME/etc/hadoop/core-site.xml
      <property>
        <name>fs.defaultFS</name>
          <value>file:///</value>
          <description>The name of the default file system.  A URI whose
          scheme and authority determine the FileSystem implementation.</description>
      </property>

      Files are accessed locally using the local file system protocol. 
      Remember no name node is running in local node. 
       
       
2. Edit the $HADOOP_HOME/etc/hadoop/hdfs-site.xml
      <property>
        <name>dfs.replication</name>
        <value>1</value>
      </property>

      This is irrelevant now, HDFS hdfs is not running for file system. 
    
    
3. Edit the $HADOOP_HOME/etc/hadoop/mapred-site.xml
      <property>
        <name>mapred.job.tracker</name>
        <value>local</value>
      </property>

      No job tracker here as Hadoop is now running at local mode but no job tracker and data node.        
      



How to set Hadoop to run in Pseudo-distributed mode
---------------------------------------------------
1. Edit the $HADOOP_HOME/conf/core-site.xml
      <property>
        <name>fs.defaultFS</name>
          <value>hdfs://localhost:54310</value>
          <description>The name of the default file system.  A URI whose
          scheme and authority determine the FileSystem implementation.</description>
      </property>

      This will tell how hadoop how to access the files. 
      Here it is using HDFS mode, the file system under the hood of Hadoop. 
      This can be changed to FTP and other implementation of Hadoop file system. 
      HDFS is one of them. 
       
       
2. Edit the $HADOOP_HOME/etc/hadoop/hdfs-site.xml
      <property>
        <name>dfs.replication</name>
        <value>1</value>
      </property>

      This will tell how hadoop the number of times it will replicate the files in HDFS.
      For a pseudo distributed the logical value is 1. 
      You can specify any value here say 2 or 5.
      But, when hadoop daemons runs it will message out with a warning that only 1 is a valid value in this mode. 
    
    
3. Edit the $HADOOP_HOME/etc/hadoop/mapred-site.xml
      <property>
        <name>mapred.job.tracker</name>
        <value>localhost:54311</value>
      </property>

      This will tell how hadoop the host and port that the MapReduce job tracker runs at. 
      If "local", then jobs are run in-process as a single map and reduce task 
      
      
      
      
  
      
      
