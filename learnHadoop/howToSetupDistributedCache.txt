How to Setup Distributed Cache in Hadoop
----------------------------------------

You don't want your map/reduce jobs making a million web service calls.
Instead, create a distributed cache.

References
----------
http://stackoverflow.com/questions/10239161/how-to-set-file-option-for-java-hadoop
http://stackoverflow.com/questions/26241628/hadoop-distributed-cache-throws-filenotfound-error
http://unmeshasreeveni.blogspot.in/2014/10/how-to-load-file-in-distributedcache-in.html

Make sure your class extends Configured and implements Tool


main calls ToolRunner.run(new Configuration(), new MyClass(), args);

public int run(String[] args) throws Excpption
{
  // creates job
  
}

Code
----
  Job job = new Job(new Configuration());
  
  Configuration conf = job.getConfiguration();
  
  job.setJarByClass(this.class)
  job.setMapperClass( )
  job.setReducerClass( )
  job setMapOutputKeyClass
  job setMapOutputValueClass
  job setInputFormatClass(XmlInputFormat)
  
  FileSystem fs = FileSystem.get(conf);
  Path cacheFiles = new Path("/path/to/lookup/file.txt");
  FileStatus[] fsList = fs.globStatus(cacheFile)
  for (FileStatus status: fsList)
  {
    DistributedCache.addCacheFile(status.getPath().toUri(), conf);
  }
  
  
  
In Reducer
----------
  private TreeMap lookupMap = new TreeMap<String, String>();

  URI[] cacheFileUris = DistributedCache.getCacheFiles(jobConfiguration)
  
  // verify that cacheFileUris is only one element
  
  Path pathCacheFile = new Path(cacheFileUris[0\.getPath() );
  
  // Open this file
  FileSystem fs = FileSystem.get(job-configuration)
  BufferedReader fileReader = new BufferedReader(new InputStreamReader(fs.open(pathCacheFile)));
  
  while ((sLine = fileReader.readLine()) != null)
   {
     // add stuff into some lookup treemap or hashmap
   }
   
  fileReader.close();
