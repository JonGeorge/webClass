Misc Hadoop Notes
-----------------
1) When creating your Hadoop Map/Reduce Job, be aware of the class path
   For example, hadoop comes with the apache commons codec jar in the hadoop/lib directory
   -- If you use a newer version of codec in your project, Hadoop will ignore it
   -- By default, the hadoop/lib gets higher precedence in the classpath
   
  
  To find out the JAR you are actually using at **run time**, try running this code:
    final URL loc = MyClass.class.getProtectionDomain().getCodeSource().getLocation();
    System.out.println("Found jar file for Myclass at ", loc);
    
    
2) Be careful with job configuration


  Bad Code:  Common mistake in your job configuration:
  ----------------------------------------------------
    Configuration conf2 = new Configuration();      
    job = new Job(conf2);
    job.setJobName("Join with Cache");
    DistributedCache.addCacheFile(new URI("hdfs://server:port/FilePath/part-r-00000"), conf2);

   After you create your Job object, you need to pull back the Configuration object as Job makes a copy of it, 
   and configuring values in conf2 after you create the job will have no effect on the job iteself. Try this:

  Good Code:  
  ---------
    job = new Job(new Configuration());
    Configuration conf2 = job.getConfiguration();
    job.setJobName("Join with Cache");
    DistributedCache.addCacheFile(new URI("hdfs://server:port/FilePath/part-r-00000"), conf2);

    NOTE:  the conf2 object is now a pointer to the Job's configuration
    
    
