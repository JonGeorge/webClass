How to Debug a Map-Reduce Job using Intellij
--------------------------------------------

Assumptions:
 A) You have Intellij Commnity Edition 14 installed
 B) You have a Java JDK
 C) You have an HDFS 
    (you followed the procedures in howToCompile AndInstallHdfsOnWindows.txt)
 D) You have hadoop running in "Local" mode
 

Procedures
----------
 1. Create your wordCount Map-Reduce Project
    a. Startup Intellij
       NOTE:  IF an existing Intellij project appear, then pull File / Close Project
       
    b. Press "Create New Project"
       1) In the "New Project" window, 
            a) Select Maven [on the left]
            b) Next to Project SDK:  Select your Java JDK
               NOTE:  If you do not see your java JDK, then
                      Press New
                      -- Browse to your Java JDK:  C:\Program Files\Java\jdk1.7.0_60
                      -- Press OK
               
            c) Check "Create from archetype"
            d) Select maven-archetype-quickstart
               Press Next

               
       2) In the next screen
          GroupId:     com.whatever
          ArtifactId:  wordCount    
          Version:     1.0-SNAPSHOT
          Press Next
          
          
       3) In the next screen,
          Maven home directory:  C:/tools/apache-maven-3.2.3
          User settings file:    Check the checkbox to "Override"
                                 C:\tools\apache-maven-3.2.3\conf\settings.xml
          Press Next                       
                                 
       
       4) Project Name:      wordCount
          Project Location:  C:\tools\intellij\workspace\wordCount
          Press Finish
          
       5) If you get prompted that "C:\tools\intellij\workspace\wordCount" does not exist
          It will be created by Intellij
          Press OK 
     
       6) If you get prompted 
          "New projects can either be opened in a new window or replace the project"
          Press "This Window"
          
       7) If you are prompted 
          Maven projects need to be imported
          Press "Enable Auto-Import"
          
          Now, maven has created a simple Java JAR project
         
 
 
 2. Configure Intellij preferences for this project
    a. Turn off spell-checking
       1) Pull File / Settings
       2) Search for spelling
          a) Single-click on Inspections
          b) next to Typo -- Uncheck the checkbox
          c) Press Apply
          
          


 3. In Intellij, exclude build directory
    a. Pull File / Settings
    b. Search for maven
       Importing -> Uncheck exclude build directory
      
     

 4. Setup the Maven test directories for your Intellij project

    Add these directories
       src/test/resources/             # Holds the logback.xml used by unit test code
       src/main/resources/             # Holds the logback.xml used by main program

    a. Right-click on src/test   -> New -> Directory:    resources
    b. Right-click on src/main   -> New -> Directory:    resources

    Now, you should have this directory structure
      src/main/java/ 
      src/main/resources/               # Holds logback.xml used by java main program     
      src/test/java
      src/test/resources/               # Holds logback.xml used by unit test code


           
 5. Add dependencies for logging and map-reduce
    Your pom.xml should already have one dependency:
      <dependencies>
      
        <dependency>
          <groupId>junit</groupId>
          <artifactId>junit</artifactId>
          <version>4.12</version>
          <scope>test</scope>
        </dependency>
        
      </dependencies>
    
        
        
    Copy and paste these dependencies into your pom.xml
    NOTE:  Add all of these dependencies should be between these tags:
       <dependencies>
       . . .
       </dependencies>
       
     
        <dependency>
           <groupId>org.apache.hadoop</groupId>
           <artifactId>hadoop-core</artifactId>
           <version>1.0.3</version>
           <scope>provided</scope>
        </dependency>
        
        <dependency>
           <groupId>org.slf4j</groupId>
           <artifactId>slf4j-api</artifactId>
           <version>1.7.5</version>
        </dependency>
        
        <dependency>
           <!-- Hadoop uses log4j so use SLF4j w/log4j bridge -->
           <groupId>org.slf4j</groupId>
           <artifactId>slf4j-log4j12</artifactId>
           <version>1.7.12</version>
       </dependency>



 6. Create this file:  log4j.xml
    NOTE:  Hadoop uses log4j so we must, too.
    
    a. Browse to src/main/resources
    b. Right-click on classes -> New -> Other... -> Search for file
       filename:  log4j.xml
       
        <?xml version="1.0" encoding="UTF-8" ?>
        <!DOCTYPE log4j:configuration SYSTEM "log4j.dtd">
        
        <log4j:configuration xmlns:log4j="http://jakarta.apache.org/log4j/">
            <appender name="CONSOLE" class="org.apache.log4j.ConsoleAppender">
                <param name="Target" value="System.out"/>
                <layout class="org.apache.log4j.PatternLayout">
                    <param name="ConversionPattern" value="%d{MM/dd/yyyy HH:mm:ss} %c %m%n"/>
                </layout>
            </appender>
        
        
            <logger name="com.whatever" additivity="false">
                <level value="debug" />
                <appender-ref ref="CONSOLE" />
            </logger>
        
            <root>
                <priority value="info" />
                <appender-ref ref="CONSOLE" />
            </root>
        
        </log4j:configuration>
   
   
   
 7. in Intellij, create your WordCount class 
 

        package com.whatever;
        
        import java.io.IOException;
        import java.util.StringTokenizer;
        
        import org.apache.hadoop.conf.Configuration;
        import org.apache.hadoop.fs.Path;
        import org.apache.hadoop.io.IntWritable;
        import org.apache.hadoop.io.Text;
        import org.apache.hadoop.mapreduce.Job;
        import org.apache.hadoop.mapreduce.Mapper;
        import org.apache.hadoop.mapreduce.Reducer;
        import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
        import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
        import org.apache.hadoop.util.GenericOptionsParser;
        import org.slf4j.Logger;
        import org.slf4j.LoggerFactory;
        
        public class WordCount
        {
            private static final Logger logger = LoggerFactory.getLogger(WordCount.class);
        
        
            public static void main(String[] args) throws Exception
            {
                logger.debug("######### main() started #######");
                Configuration conf = new Configuration();
                String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
                if (otherArgs.length < 2)
                {
                    System.err.println("Usage: wordcount <in> [<in>...] <out>");
                    System.exit(2);
                }
        
                Job job = new Job(conf, "word count");
                job.setJarByClass(WordCount.class);
                job.setMapperClass(TokenizerMapper.class);
                job.setCombinerClass(IntSumReducer.class);
                job.setReducerClass(IntSumReducer.class);
                job.setOutputKeyClass(Text.class);
                job.setOutputValueClass(IntWritable.class);
                for (int i = 0; i < otherArgs.length - 1; ++i)
                {
                    FileInputFormat.addInputPath(job, new Path(otherArgs[i]));
                }
        
                FileOutputFormat.setOutputPath(job,
                        new Path(otherArgs[otherArgs.length - 1]));
                boolean bJobFinishedSuccessfully = job.waitForCompletion(true);
        
                logger.debug("############## job finished successfully?  {}", bJobFinishedSuccessfully);
            }
        
        
            //  M A P P E R       C L A S S
            public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {
        
                private final static IntWritable one = new IntWritable(1);
                private Text word = new Text();
        
                public void map(Object key, Text value, Context context) throws IOException, InterruptedException
                {
                    StringTokenizer itr = new StringTokenizer(value.toString());
                    while (itr.hasMoreTokens())
                    {
                        word.set(itr.nextToken());
                        context.write(word, one);
                    }
                }
            }
        
        
           //  R E D U C E R       C L A S S
           public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable>
            {
                private IntWritable result = new IntWritable();
        
                public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException
                {
                    int sum = 0;
                    for (IntWritable val : values)
                    {
                        sum += val.get();
                    }
                    result.set(sum);
                    context.write(key, result);
                }
            }
        
        }


 
 8. Setup your input directory 
    a. Create the c:/temp/input directory
    b. Create a file called c:\temp\input\stuff.txt with this
            hi mom
            hi Ben
            hi Peter
            hi Dad
            hi Will
            hi Sam
            Will is cool
      
      
           
8b. **OPTIONAL** 
    Only use this step if running Hadoop in pseudo-distributed mode
    
    Setup the HDFS directories and test data
    a. Create the HDFS directorires /tmp/input 
       DOS> hadoop fs -mkdir /tmp/input
       DOS> hadoop fs -rmdir /tmp/output
       
    b. Create a file called c:\temp\stuff.txt with this
            hi mom
            hi Ben
            hi Peter
            hi Dad
            hi Will
            hi Sam
            Will is cool
    
    c. Put that file in the /tmp/input direcory in HDFS
       DOS> hadoop fs -put c:\temp\stuff.txt /tmp/input/stuff.txt
      
      
      
9. In Intellij, setup Remote debugging
    a. Pull Run / Edit Configurations
    b. Click the "+" on the left
    c. Click on "Remote"
       Name:           remote-wordCount
       Transport:      Socket
       Debugger mode:  Attach
       Host:           localhost
       Port:           5005
       Press Apply
       Press OK
       
   
   
10. Build your code -- create java jar
    a. Open a DOS window by pressing <Start><Run>CMD
    b. In the DOS window, type-in this:
       DOS> cd /d c:\tools\intellij\workspace\wordCount
       DOS> mvn clean package
       
       Now, you have this JAR
          c:\tools\intellij\workspace\wordCount\target\wordCount-1.0-SNAPSHOT.jar
     
       
       
11. Run your hadoop java jar in debug mode
    a. Make sure your test data is setup
       If running in "Local" mode, then make sure you have a file in c:\temp\input
       If running in "Psuedo-distributed" mode, then 
         -- Make sure your HDFS system is turned on
         -- Make sure your have a /tmp/input/stuff.txt file in your HDFSMake sure your HDFS system is turned on
             
    b. Open a DOS window in Administrative mode
       Run these commands
       
       1) Setup Hadoop to run in debug mode   
          DOS> set HADOOP_OPTS=-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005
       
       2) Setup HADOOP_CLASSPATH so that it can find your log4j.xml file
          DOS> set HADOOP_CLASSPATH=c:\tools\intellij\workspace\wordCount\src\main\resources
    
       3) If running Hadoop in "Local" mode
          Run your hadoop job in debugging mode 
          DOS> cd /d c:\tools\intellij\workspace\wordCount
          DOS> hadoop jar .\target\wordCount.jar com.whatever.WordCount c:/temp/input c:/temp/output
      
      
       3b) If running Hadoop in "Pseudo-distributed" mode
           DOS> cd /d c:\tools\intellij\workspace\wordCount
           DOS> hadoop jar .\target\wordCount.jar com.whatever.WordCount /tmp/input /tmp/output
        
           You should see Listening for transport dt_socket at address 5005
      
      
      
12. In Intellij, connect to the hadoop job remotely
    a. Set a breakpoint
    b. Pull Run / Debug remote-wordCount
    


13. Look at the results
    
    If running Hadoop in "Local" mode, then your results are in c:\temp\output
    
    
    If running Hadoop in "Pseudo-distributed" mode, then your results are in /tmp/output in HDFS
      DOS> hadoop fs -ls /tmp/output
      DOS> hadoop fs -cat /tmp/output/part-00000
            Ben     1
            Dad     1
            Peter   1
            Sam     1
            Will    2
            cool    1
            hi      6
            is      1
            mom     1


In order to run again
 -- Delete the hdfs /tmp/output directory
    DOS> hadoop fs -rm -r /tmp/output