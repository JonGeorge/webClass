Unicode Explained
-----------------

References
----------
https://hadoopoopadoop.com/2015/12/05/no-fluff-unicode-sumary-for-hadoop/
https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/


Key Points
----------
 A) The most widely used Unicode encoding today, by far, is UTF-8, but UTF-16 is not dead yet.
 B) Sometimes you are forced by circumstance to ingest UTF-16, but the only reason to write any format other than UTF-8 is to accommodate legacy processing.
 C)  Occasionally, other formats, e.g., UTF-32, are used for special purposes internally to some program. If you need to know about this, then you are beyond needing this primer.
 D) UTF-8 and UTF-16 are both “variable length” encodings, i.e., not every character is expressed with the same number of bytes.
 E) ASCII by definition is 7-bit.
    Range is 0 through FF hex, which includes FF+1 values or, in decimal, 128.
     If the high order bit is set, it’s not ASCII.
 F) UTF-16 and UTF-8 represent the ASCII characters with the same numeric values used in ASCII, but they encode them differently.
 
 G) UTF-16 always uses either two bytes or four bytes.
          ASCII characters will have an extra all-zero byte in addition to the byte with the ASCII value.
          Whether the all-zero byte is leading or trailing depends on the endian-ness of the representation.
          The BMP characters all fit into 16 bits.
 H) UTF-8 uses:
            1 byte for code points < 8 bits (ASCII characters, i.e. the Latin alphabet)
            2 bytes for all code points that require from 8 to 11 bits
            3 bytes for all code points that require from 12 to 16 bits
            4 bytes for all code points that require from 17 to 21 bits
            Note that this implies that much of the BMP requires three bytes, not two.

