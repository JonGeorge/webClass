How to Sync an ElasticSearch Index to a Postgres Database Table
---------------------------------------------------------------
The database table holds the "record of truth" and as the database table is updated,
we want to update an ElasticSearch index.  But, we do not want to do it in real-time


INCOMPLETE
    a) Needs PostgresDatasource singleton
    b) Needs to implement algorithm with SQL transaction



Assumptions:
 A) You have an Elastic Search instance established
 B) You have a Postgres database table
 C) As you add, modify, and delete records from the database table, 
    you want to add, modify, and delete records from the ElasticSearch index


References
----------
https://qafoo.com/blog/086_how_to_synchronize_a_database_with_elastic_search.html
https://www.elastic.co/blog/found-keeping-elasticsearch-in-sync
http://dba.stackexchange.com/questions/27688/locking-issue-with-concurrent-delete-insert-in-postgresql

Design
------
 1) Create a documents table (holds the "record of truth")
 
 2) Create a changes table
 
 3) Add some sample records to the documents table
 
 4) Add some records to the changes table
 
 5) Setup a cron job that will periodically update ElasticSearch
    Perhaps, this job will run once-every-hour or once-every-5-minutes
    
    Afterwards we can just query the changes like:
		SELECT
		    sequence_number, document.*
		FROM changes
		JOIN -- … --
		WHERE sequence_number > :since
		ORDER BY sequence_number ASC
		LIMIT 0, 100;


    
    
Part 1:  Setup the Database Table and ElasticSearch Index
----------------------------------------------------------
 1. Setup the Postgres database tables
 
 	a. Create this table:  documents
 	   NOTE:  The "serial" type causes a few things to happeN:
 	            a) It creates a sequence called "documents_id_seq"
 	            b) It causes the id column to use the default value of the sequence
 	                   create table documents
 	                    (
 	                   		id INT NOT NULL DEFAULT NEXTVAL('documents_id_seq'), 
 	       					...
 	       				);
 	       				
 	       				
 	
			CREATE TABLE documents
			(
			  id      serial,
			  title   varchar(50) not null,
			  author  varchar(50) not null,
			  summary varchar(50) null,
			  primary key(id)
			);

 		
 	b. Create this table:  changes

			CREATE TYPE changetype AS ENUM('create','delete','update');
			CREATE TABLE changes
			(
			  id      serial,
			  docid   int not null,
		      type    changetype NOT NULL
			);

 
  	c. Add some sample records to the documents and changes tables
 			insert into documents(title, author) values('The Wizard of Oz', 'someone');
 			insert into documents(title, author) values('Hitchhiker''s Guide to the Galaxy', 'Douglas Adams');
 			insert into changes(docid, type) values(1, 'create');
 			insert into changes(docid, type) values(2, 'create');
 			
 
 
 2. Setup a mapping in ElasticSearch called docs
 	a. Startup Chrome
 	b. Startup the "Sense" plugin
 	c. Run the following commands:	
	 
		1) Delete the index called "docs"
	    	DELETE /docs
	
	
	 	2) Create a mappings for the index called "docs"
		    PUT /docs
	    	{
		       "mappings": {
		          "record": {
		             "properties": {
		                "title": {
		                   "type": "string",
		                   "analyzer": "snowball",
		                   "copy_to": "search_text" 
		                },
		                "type": {
		                   "type": "string",
		                   "index": "not_analyzed"
		                },
		                "createDate": {
		                   "type": "string",
		                   "index": "not_analyzed"
		                },
		                "description": {
		                   "type": "string",
		                   "analyzer": "snowball",
		                   "copy_to": "search_text" 
		                },
		                "search_text": {
		                   "type": "string",
		                   "analyzer": "standard"
		                }
		             }
		          }
		       }
		    }
 


Part 2:  Create this Java program that will process 1000 records from the changes table 
--------------------------------------------------------------------------------------- 
 1. Create a Java command-line project
    [see learnJava / howToCreateJavaCommandLineProgramUsingIntellijMaven.txt]
 
 
 2. Add these dependencies to your pom.xml

      <dependency>
          <groupId>ch.qos.logback</groupId>
          <artifactId>logback-classic</artifactId>
          <version>1.0.13</version>
      </dependency>

      <dependency>
          <groupId>com.google.guava</groupId>
          <artifactId>guava</artifactId>
          <version>19.0</version>
      </dependency>

      <dependency>
          <groupId>org.elasticsearch</groupId>
          <artifactId>elasticsearch</artifactId>
          <version>2.3.3</version>
      </dependency>

      <dependency>
          <!-- Use this to get Jackson's ObjectMapper -->
          <groupId>com.fasterxml.jackson.core</groupId>
          <artifactId>jackson-databind</artifactId>
          <version>2.1.0</version>
      </dependency>
      
      
      WARNING:  
        If running ElasticSearch *AND* using Apache Storm,
        Then, you will need to exclude the org.jboss.netty to connect to ElasticSearch properly.
             
     		 So, your Apache Storm dependency would look like this:
		      <dependency>
		          <groupId>org.apache.storm</groupId>
		          <artifactId>storm-core</artifactId>
		          <version>0.9.2-incubating</version>
		          <scope>compile</scope>
		          <exclusions>
		               <exclusion>
		               	  <!-- Storm comes with logback-classic.  Let's exclude it -->
			              <exclusion>
			                  <groupId>ch.qos.logback</groupId>
			                  <artifactId>logback-classic</artifactId>
			              </exclusion>
			              
		                  <!-- Storm comes with jboss.netty and io.netty.  This removed the jboss.netty ones -->
		                  <groupId>org.jboss.netty</groupId>
		                  <artifactId>netty</artifactId>
		              </exclusion>
		          </exclusions>
		      </dependency>
             


 3. Update your logback.xml file to look like this:
    a. Edit /src/main/resources/logback.xml
    b. Replace your logback.xml with this:
    
		<?xml version="1.0" encoding="windows-1252" ?>
		<!DOCTYPE project>
		
		<configuration debug="false">
		    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
		        <encoder>
		            <pattern>%d{MM/dd/yyyy HH:mm:ss} %-5level %c %m%n</pattern>
		        </encoder>
		    </appender>
		
		
		
		    <logger name="com.wahtever" level="DEBUG" additivity="false">
		        <appender-ref ref="CONSOLE"/>
		    </logger>
		
		    <root level="DEBUG">
		        <appender-ref ref="CONSOLE"/>
		    </root>
		</configuration>
              
 
 
 4. Create this class:  EsClientSingleton
    a. Right-click on /src/main/java -> New -> Java Class
       Name:  EsClientSingleton
       Kind:  Class
       Press OK
       
    b. Copy this to EsClientSingleton.java 
		    
			package com.whatever;
			
			import org.elasticsearch.client.Client;
			import org.elasticsearch.client.transport.TransportClient;
			import org.elasticsearch.common.transport.InetSocketTransportAddress;
			import java.net.InetAddress;
			
			/**
			 * Created by adam on 6/25/16.
			 */
			public enum EsClientSingleton
			{
			    INSTANCE;
			
			    private Client esClient;
			
			    /******************************************************************
			     * Private EsClientSingleton Constructor
			     * -- Do one-time initialization here
			     *******************************************************************/
			    private EsClientSingleton()
			    {
			        // The very first time this singleton is called, we will initialize the ES Client
			        initializeEsClientNoException();
			    }
			
			
			    /******************************************************************
			     * initializeEsClientNoException()
			     * -- Attempt to connect to the ElasticSearch Instance
			     *******************************************************************/
			    private void initializeEsClientNoException()
			    {
			        try {
			            // Connect to the ElasticSearch instance using port 9300  [it's a binary protocol]
			            // NOTE:  The java client does not connect to port 9200
			            InetAddress addrEsHostname = InetAddress.getByName("localhost");
			
			            esClient = TransportClient.builder().build()
			                    .addTransportAddress(new InetSocketTransportAddress(addrEsHostname, 9300));
			        }
			        catch (Exception e)
			        {
			            // Convert the exception to a RunTime exception
			            RuntimeException re = new RuntimeException(e);
			            re.setStackTrace(e.getStackTrace());
			            throw re;
			        }
			
			    }
			
			    public Client getEsClient()
			    {
			        return esClient;
			    }
			}
		

				
 5. Create this class:  AddRecords
    a. Right-click on /src/main/java -> New -> Java Class
       Name:  SyncRecords
       Kind:  Class
       Press OK
       
    b. Copy this to SyncRecords.java 

			package com.whatever;
			
			import com.fasterxml.jackson.databind.ObjectMapper;
			import org.elasticsearch.action.admin.indices.create.CreateIndexRequest;
			import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;
			import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;
			import org.elasticsearch.action.admin.indices.delete.DeleteIndexResponse;
			import org.elasticsearch.action.index.IndexRequest;
			import org.elasticsearch.action.index.IndexResponse;
			import org.elasticsearch.action.search.SearchResponse;
			import org.elasticsearch.client.Client;
			import org.elasticsearch.common.settings.Settings;
			import org.elasticsearch.common.xcontent.XContentBuilder;
			import org.elasticsearch.index.IndexNotFoundException;
			import org.elasticsearch.indices.IndexAlreadyExistsException;
			import org.elasticsearch.search.SearchHit;
			import org.slf4j.Logger;
			import org.slf4j.LoggerFactory;
			import static org.elasticsearch.common.xcontent.XContentFactory.*;
			
			import java.util.HashMap;
			import java.util.Map;
			
			/**
			 * SyncRecords
			 *
			 */
			public class SyncRecords
			{
			    public static final Logger logger = LoggerFactory.getLogger(SyncRecords.class);
			
			    public static void main( String[] args ) throws Exception
			    {
			        logger.debug("main() started");
			
			
			        // Connect to the ElasticSearch instance and return the client
			        Client esClient = EsClientSingleton.INSTANCE.getEsClient();
			
			        final String ES_INDEX_NAME = "docs";
			
			
				
			        // A T T E M P T      T O     C R E A T E      I N D E X    (within ElasticSearch)
			        try
			        {
			            // Create an index called "music"
			            logger.debug("Attempting create the index called '{}'....", ES_INDEX_NAME);
			            Settings indexSettings = Settings.builder()
			                    .put("number_of_shards", 1)
			                    .put("number_of_replicas", 1)
			                    .build();
			            CreateIndexRequest indexRequest = new CreateIndexRequest(ES_INDEX_NAME, indexSettings);
			            CreateIndexResponse res = esClient.admin().indices().create(indexRequest).actionGet();
			            if (! res.isAcknowledged())
			            {
			                logger.error("I failed to create this index:  {}", ES_INDEX_NAME);
			            }
			        }
			        catch (IndexAlreadyExistsException alreadyExistsException)
			        {
			            // Ignore this exception
			            logger.debug("Ignoring IndexAlreadyExistsException that was raised.");
			        }
			
			        IndexResponse res;
			
			
			        // Add Record Approach #1:  Use a HashMap
			        Map<String, Object> jsonMap = new HashMap<String, Object>();
			        jsonMap.put("createDate", "02/25/2016");
			        jsonMap.put("description", "This is the description for record #1");
			        jsonMap.put("author", "Adam");
			
			        // Create the record and set _type="record" and _id="1"
			        // NOTE:  By setting the ID you can only create this record once.  Calling it multiple times does not add additional records
			        IndexRequest indexRequest = new IndexRequest(ES_INDEX_NAME, "record", "1")
			                                        .source(jsonMap);
			        res = esClient.index(indexRequest).actionGet();
			        logger.debug("After adding record #1. res.isCreated={}  res.getId()={}", res.isCreated(), res.getId());
			
			
			
			        // Add Record Approach #2:  Use a JSON string
			        String sJson = "{  \"createDate\": \"02/26/2016\",        \"description\": \"This is the description for record #2\", \"author\": \"Ben\" }";
			        IndexRequest indexRequest2 = new IndexRequest(ES_INDEX_NAME, "record", "2")
			                                        .source(sJson);
			        res = esClient.index(indexRequest2).actionGet();
			        logger.debug("After adding record #2. res.isCreated={}  res.getId()={}", res.isCreated(), res.getId());
			
			
			
			        // Add Record Approach #3:  Use ElasticSearch helpers to generate JSON
			        XContentBuilder builder = jsonBuilder()
			            .startObject()
			                .field("createDate", "02/28/2016")
			                .field("desciption", "This is the description for record #3")
			                .field("author",     "Justin")
			            .endObject();
			
			        String sJson3 = builder.string();
			        IndexRequest indexRequest3 = new IndexRequest(ES_INDEX_NAME, "record", "3")
			                .source(sJson3);
			        res = esClient.index(indexRequest3).actionGet();
			        logger.debug("After adding record #3. res.isCreated={}  res.getId()={}", res.isCreated(), res.getId());
			
			
			
			        // Add Record Approach #4:  Serialize your object and write the object as a byte[]
			        // NOTE:  Requires adding pom.xml dependency:   com.fasterxml.jackson.core:jackson-databind:2.1.0
			        MyRecord myRecord = new MyRecord("03/01/2016", "This is the description for record #4", "Peter");
			        ObjectMapper mapper = new ObjectMapper(); // create once, reuse
			        byte[] jsonByteArray = mapper.writeValueAsBytes(myRecord);
			        IndexRequest indexRequest4 = new IndexRequest(ES_INDEX_NAME, "record", "4")
			                .source(jsonByteArray);
			        res = esClient.index(indexRequest4).actionGet();
			        logger.debug("After adding record #4. res.isCreated={}  res.getId()={}", res.isCreated(), res.getId());
			
			
			
			        // Search for the records  (you should get 4 hits)
			        SearchResponse searchResponse =
			                esClient.prepareSearch(ES_INDEX_NAME).setTypes("record").execute().actionGet();
			        SearchHit[] hits = searchResponse.getHits().getHits();
			        logger.debug("After running search.  hits.length={}", hits.length);
			
			
			
			       // esClient.admin().indices();
			       logger.debug("main() finished");
			    }
			}
			
    
    
 
// Our worker, scheduled to run every N minutes.
// It will read 1000 entries off the queue, issue a single batch request
// to elasticsearch, and repeat if more items are still on the queue
class QueueWorker < Worker {
  function work() {
    errorCount = 0;
    // Transactionally reserve 1000 entries at a time, but don't delete them from the queue yet
    // The entries will be only be fully deleted once the bulk operation has successfully completed
    while ((queue_entries = Queue.reserve(limit: 1000)) && records.length > 0) {
      try {
        // In this example we will build up the JSON-like body for the 
        // elasticsearch queue API request. It consists of newline separated
        // JSON documents comprising action metadata, and document values
        bulk_body = "";
        record_class = Class.named(queue_entry.record_type);
        i = 0;  
        queue_entries.each(function (queue_entry) {
          i++;
          record = record_class.find(id: queue_entry.record_id);
       
          // Note, a production ready version of this code would double-check that the document still exists
          // and would create a bulk delete request if the record was no longer present
          action_metadata = {index: {
                              _index: record_class.elasticsearch_index,
                              _type: record_class.elasticsearch_type,
                              _id: record.id}}
       
          bulk_body += "\n" unless i == 1;
          bulk_body += encode_json(action_metadata);
          bulk_body += "\n";            
          bulk_body += encode_json(record);
        });
        
        bulk_body += "\n"; // The bulk API requires termination by a newline  
        http_client.post("http://host.for.elasticsearch:9200/_bulk", body: bulk_body);
        // Now that we're sure processing has succeeded we can fully delete the queue entries
        queue_entries.delete
      } catch (StandardError ex) {
        queue_entries.unreserve;
        
        // Simply let the while loop retry up to 5 times.
        errorCount += 1
        if (errorCount >= 5) {
          throw(CannotReplicateElasticsearchError);
        } else {
          Logger.warn("Error processing elasticsearchqueue, will retry. Attempt: $errorCount", ex)
        }
      }
    }
  }
}
 
 
 
 
Part 3:  Run the program
------------------------
 1. Query against the ElasticSearch index (there should be no records)
 	a. Startup Chrome
 	b. Run the "Sense" plugin
 	c. Run this command:
 			GET /docs/_search
 			
 
 2. Run the Java Program
 
 
 
 3. Query against the ElasticSearch index (there should be a few records)
  	a. Startup Chrome
 	b. Run the "Sense" plugin
 	c. Run this command:
 			GET /docs/_search
 