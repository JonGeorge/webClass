How to Sync a Postgres Database Table w/ElasticSearch 1.7.5 Index 
-----------------------------------------------------------------
The database table holds the "record of truth" and as the database table is updated,
we want to update an ElasticSearch index.  But, we do not want to do it in real-time

As you add, modify, and delete records from the database table, 
we want to *PERIODICALLY* add, modify, and delete records from the ElasticSearch index
    
    

Assumptions:
 A) You have an Elastic Search instance 1.7.5 up & listening at 192.168.1.157 on port 9200/9300
    [see howToInstallElasticSearch1.7.5OnCentosUsingRpm.txt]
    
 B) You have a Postgres database table up & listening at 192.168.1.157 on port 5432
    [see howToInstallPostgresOnCentOS.txt]
    



References
----------
https://qafoo.com/blog/086_how_to_synchronize_a_database_with_elastic_search.html
https://www.elastic.co/blog/found-keeping-elasticsearch-in-sync
http://dba.stackexchange.com/questions/27688/locking-issue-with-concurrent-delete-insert-in-postgresql
http://stackoverflow.com/questions/16920902/elasticsearch-java-bulk-batch-size

    
    
Part 1:  Setup the Database Table and ElasticSearch Index
----------------------------------------------------------
 1. Setup the Postgres database tables
 	a. Create this table:  documents
 	   NOTE:  The "serial" type causes a few things to happeN:
 	            a) It creates a sequence called "documents_id_seq"
 	            b) It causes the id column to use the default value of the sequence
 	                   create table documents
 	                    (
 	                   		id INT NOT NULL DEFAULT NEXTVAL('documents_id_seq'), 
 	       					...
 	       				);
 	       				
 	       				
 	
			CREATE TABLE documents
			(
			  id      serial,
			  title   varchar(50) not null,
			  author  varchar(50) not null,
			  summary varchar(50) null,
			  primary key(id)
			);

 		
 	b. Create this table:  changes

			CREATE TYPE changetype AS ENUM('create','delete','update');
			CREATE TABLE changes
			(
			  id      serial,
			  docid   int not null,
		      type    changetype NOT NULL
			);

 
  	c. Add some sample records to the documents and changes tables
 			insert into documents(title, author) values('The Wizard of Oz', 'someone');
 			insert into documents(title, author) values('Hitchhiker''s Guide to the Galaxy', 'Douglas Adams');
 			insert into changes(docid, type) values(1, 'create');
 			insert into changes(docid, type) values(2, 'create');
 			
 
 
 2. Setup a mapping in ElasticSearch called docs
 	a. Startup Chrome
 	b. Startup the "Sense" plugin
 	c. Run the following commands:	
	 
		1) Delete the index called "docs"
	    	DELETE /docs
	
	
	 	2) Create a mappings for the index called "docs"
		    PUT /docs
	    	{
		       "mappings": {
		          "record": {
		             "properties": {
		                "title": {
		                   "type": "string",
		                   "analyzer": "snowball",
		                   "copy_to": "search_text" 
		                },
		                "source": {
		                   "type": "string",
		                   "index": "not_analyzed"
		                },
		                "ingestDate": {
		                   "type": "date",
		                   "format": "yyyyMMdd",
		                   "index": "not_analyzed"
		                },
		                "createDate": {
		                   "type": "date",
		                   "format": "yyyyMMdd",
		                   "index": "not_analyzed"
		                },	
		                "description": {
		                   "type": "string",
		                   "analyzer": "snowball",
		                   "copy_to": "search_text" 
		                },
		                "search_text": {
		                   "type": "string",
		                   "analyzer": "standard"
		                }
		             }
		          }
		       }
		    }
 

	 	3) Delete any existing records (but keep the mapping)
	    	DELETE /docs/record
 
 
		4) Add some sample records to this index
	    	POST _bulk
	    	{ "create": { "_index": "docs", "_type": "record"}}
	    	{ "title": "Record #1", "source": "log format1", "ingestDate", "20150526", "createDate": "20160124", "description": "This is the description for record #1" }
	    	{ "create": { "_index": "docs", "_type": "record"}}
	    	{ "title": "Record #2", "source": "log format2", "createDate": "02/25/2016", "description": "This is the description for record #2"}
	    	{ "create": { "_index": "docs", "_type": "record"}}
	    	{ "title": "Record #3", "source": "log format2", "createDate": "02/25/2016", "description": "This is the description for record #3"}
    
    
    	5) View all records in this index
	    	GET /docs/record/_search
    
    

Part 2:  Create this Java program that will process 1000 records from the changes table 
--------------------------------------------------------------------------------------- 
 1. Create a Java command-line project
    [see learnJava / howToCreateJavaCommandLineProgramUsingIntellijMaven.txt]
 
 
 2. Add these dependencies to your pom.xml


      <dependency>
          <!-- Used to convert objects to JSON -->
          <groupId>com.google.code.gson</groupId>
          <artifactId>gson</artifactId>
          <version>2.8.0</version>
      </dependency>

      <dependency>
          <!-- Used for my httpClient implementation -->
          <groupId>com.ning</groupId>
          <artifactId>async-http-client</artifactId>
          <version>1.9.31</version>
      </dependency>

       <dependency>
           <!-- Spring Core -->
          <groupId>org.springframework</groupId>
          <artifactId>spring-core</artifactId>
          <version>4.1.1.RELEASE</version>
          <exclusions>
              <exclusion>
                  <groupId>commons-logging</groupId>
                  <artifactId>commons-logging</artifactId>
              </exclusion>
          </exclusions>
      </dependency>

      <dependency>
          <!-- Used to get the Spring JDBC Single Connection Data Source -->
          <groupId>org.springframework</groupId>
          <artifactId>spring-jdbc</artifactId>
          <version>4.1.1.RELEASE</version>
      </dependency>

      <dependency>
          <!-- Add the dependency for your postgreSQL driver -->
          <groupId>org.postgresql</groupId>
          <artifactId>postgresql</artifactId>
          <version>9.4.1212.jre7</version>
      </dependency>

      <dependency>
          <!-- Bridge Logging from Java Common Logging to SLF4J -->
          <groupId>org.slf4j</groupId>
          <artifactId>jcl-over-slf4j</artifactId>
          <version>1.7.5</version>
      </dependency>

      <dependency>
          <!-- Logback is our SLF4j logging implementation -->
          <groupId>ch.qos.logback</groupId>
          <artifactId>logback-classic</artifactId>
          <version>1.0.13</version>
      </dependency>
      

 3. Update your logback.xml file to look like this:
    a. Edit /src/main/resources/logback.xml
    b. Replace your logback.xml with this:
    
		<?xml version="1.0" encoding="windows-1252" ?>
		<!DOCTYPE project>
		
		<configuration debug="false">
		    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
		        <encoder>
		            <pattern>%d{MM/dd/yyyy HH:mm:ss} %-5level %c %m%n</pattern>
		        </encoder>
		    </appender>
		
		
		    <logger name="stuff" level="DEBUG" additivity="false">
		        <appender-ref ref="CONSOLE"/>
		    </logger>
		
		    <logger name="org.elasticsearch" level="INFO" additivity="false">
		        <appender-ref ref="CONSOLE"/>
		    </logger>
		
		    <logger name="org.springframework.jdbc" level="DEBUG" additivity="false">
		        <appender-ref ref="CONSOLE"/>
		    </logger>
		
		    <root level="INFO">
		        <appender-ref ref="CONSOLE"/>
		    </root>
		
		</configuration>
              
 
 4. Create a package called "stuff"
    a. Right-click on /src/main/java -> New -> Package
       Name:  Stuff
       
 
 5. Create this class:  EsClientSingleton
    a. Right-click on /src/main/java/stuff -> New -> Java Class
       Name:  EsClientSingleton
       Kind:  Class
       Press OK
       
    b. Copy this to to your java class
		    
		package stuff;
		
		import org.elasticsearch.client.Client;
		import org.elasticsearch.client.transport.TransportClient;
		import org.elasticsearch.common.transport.InetSocketTransportAddress;
		import java.net.InetAddress;
		
		/**
		 * Created by adam on 6/25/16.
		 */
		public enum EsClientSingleton
		{
		    INSTANCE;
		
		    private Client esClient;
		
		    /******************************************************************
		     * Private EsClientSingleton Constructor
		     * -- Do one-time initialization here
		     *******************************************************************/
		    private EsClientSingleton()
		    {
		        // The very first time this singleton is called, we will initialize the ES Client
		        initializeEsClientNoException();
		    }
		
		
		    /******************************************************************
		     * initializeEsClientNoException()
		     * -- Attempt to connect to the ElasticSearch Instance
		     *******************************************************************/
		    private void initializeEsClientNoException()
		    {
		        try {
		            // Connect to the ElasticSearch instance using port 9300  [it's a binary protocol]
		            // NOTE:  The java client does not connect to port 9200
		            InetAddress addrEsHostname = InetAddress.getByName("192.168.1.157");

			        this.esClient = new TransportClient()
			                .addTransportAddress(new InetSocketTransportAddress(addrEsHostname, 9300));
		        }
		        catch (Exception e)
		        {
		            // Convert the exception to a RunTime exception
		            RuntimeException re = new RuntimeException(e);
		            re.setStackTrace(e.getStackTrace());
		            throw re;
		        }
		
		    }
		
		    public Client getEsClient()
		    {
		        return esClient;
		    }
		}


				
 6. Create this class:  PostgresSingleton
    a. Right-click on /src/main/java/stuff -> New -> Java Class
       Name:  PostgresSingleton
       Kind:  Class
       Press O
       
    b. Copy this to to your java class
    
		package stuff;
		
		import org.slf4j.Logger;
		import org.slf4j.LoggerFactory;
		import org.springframework.jdbc.core.JdbcTemplate;
		import org.springframework.jdbc.datasource.SingleConnectionDataSource;
		
		import javax.sql.DataSource;
		
		/**
		 * Created by adam on 10/30/2016.
		 */
		public enum PostgresSingleton
		{
		    INSTANCE;
		
		    private DataSource datasource;
		
		    /******************************************************************
		     * Private PostgresSingleton Constructor
		     * -- Do one-time initialization here
		     *******************************************************************/
		    private PostgresSingleton()
		    {
		        // The very first time this singleton is called, we will initialize the data source by attempting to run a query
		        initializeDataSourceNoException();
		    }
		
		
		    /******************************************************************
		     * initializeDataSourceNoException()
		     * -- Attempt to connect to the Postgres Data Source
		     *******************************************************************/
		    private void initializeDataSourceNoException()
		    {
		        try {
		            // Create the data source object
		            SingleConnectionDataSource sds = (SingleConnectionDataSource) new SingleConnectionDataSource();
		
		            // Initialize the data source by setting its properties
		            sds.setDriverClassName("org.postgresql.Driver");
		            sds.setUrl("jdbc:postgresql://192.168.1.157:5432/postgres");
		            sds.setUsername("postgres");
		            sds.setPassword("secret");
		
		            this.datasource = sds;
		
		            JdbcTemplate jt = new JdbcTemplate(sds);
		            final String sSql = "select now()";
		            String sCurrentDateTime = jt.queryForObject(sSql, String.class);
		        }
		        catch (Exception e)
		        {
		            // Convert the exception to a RunTime exception
		            RuntimeException re = new RuntimeException(e);
		            re.setStackTrace(e.getStackTrace());
		            throw re;
		        }
		
		    }
		
		    public DataSource getDataSource()
		    {
		        return datasource;
		    }
		}
		 
 7. Create this class:  SyncRecords
    a. Right-click on /src/main/java/stuff -> New -> Java Class
       Name:  SyncRecords
       Kind:  Class
       Press OK
       
       
    b. Copy this to to your java class
    
		package stuff;
		
		import com.google.gson.Gson;
		import com.google.gson.JsonElement;
		import com.google.gson.JsonObject;
		import com.google.gson.JsonParser;
		import com.ning.http.client.AsyncHttpClient;
		import com.ning.http.client.Response;
		import org.slf4j.Logger;
		import org.slf4j.LoggerFactory;
		import org.springframework.jdbc.core.JdbcTemplate;
		import org.springframework.jdbc.datasource.DataSourceTransactionManager;
		import org.springframework.jdbc.support.rowset.SqlRowSet;
		import org.springframework.transaction.TransactionStatus;
		import org.springframework.transaction.support.TransactionCallbackWithoutResult;
		import org.springframework.transaction.support.TransactionTemplate;
		import javax.sql.DataSource;
		
		import java.util.ArrayList;
		import java.util.HashMap;
		
		/**
		 * SyncRecords
		 *
		 */
		public class SyncRecords
		{
		    public static final Logger logger = LoggerFactory.getLogger(SyncRecords.class);
		
		    public static void main( String[] args ) throws Exception
		    {
		        logger.debug("main() started");
		
		        // Connect to the Postgres DataSource instance and return the data source object
		        final DataSource datasource = PostgresSingleton.INSTANCE.getDataSource();
		
		        // Setup an http client object [we will use this to make the actual POST calls]
		        final AsyncHttpClient httpClient = new AsyncHttpClient();
		
		        // Setup a SQL Transaction
		        TransactionTemplate tt = new TransactionTemplate();
		        tt.setTransactionManager(new DataSourceTransactionManager(datasource));
		
		        // This transaction will throw a TransactionTimedOutException after 60 seconds (causing the transaction to rollback)
		        tt.setTimeout(120);
		
		        final String ES_URL          = "http://192.168.1.165:9200";
		        final String ES_INDEX_NAME   = "docs";
		        final String ES_MAPPING_NAME = "record";
		        final int    TOTAL_RECORDS_TO_PROCESS_AT_ONCE = 3;
		
		        tt.execute(new TransactionCallbackWithoutResult()
		        {
		            protected void doInTransactionWithoutResult(TransactionStatus aStatus)
		            {
		                try
		                {
		                    boolean bChangeRecordsWereFound = false;
		                    StringBuilder sbJsonBulkRequest = new StringBuilder();
		
		                    ArrayList<String> changeIds = new ArrayList<String>();
		
		                    // Reserve 1000 entries at a time
		                    // NOTE:  If another process calls select...for update, then that other process is blocked until this transaction is finished
		                    //        If another process calls delete on these ids, then that other process is blocked until this transaction is finished
		                    //        If another process calls select...., then that other process can see the records
		                    final String sSql = "select id, docid, type " +
		                                        "from changes " +
		                                        "where id IN " +
		                                        "(select id from changes limit " + TOTAL_RECORDS_TO_PROCESS_AT_ONCE + ") " +
		                                        "order by id " +
		                                        "FOR UPDATE";
		
		                    // Run the query, get a read-only recordset, and return the connection back to the pool
		                    JdbcTemplate jt = new JdbcTemplate(datasource);
		                    SqlRowSet rs = jt.queryForRowSet(sSql);
		
		                    Gson gson = new Gson();
		
		                    // Generate the 1st line of JSON (used for each 'create' request)
		                    HashMap<String, String> innerMapCreate = new HashMap<String, String>();
		                    innerMapCreate.put("_index", ES_INDEX_NAME);
		                    innerMapCreate.put("_type",  ES_MAPPING_NAME);
		                    HashMap<String, Object> outerMapCreate = new HashMap<String, Object>();
		                    outerMapCreate.put("create", innerMapCreate);
		                    final String sCreateLine1 = gson.toJson(outerMapCreate);
		                    logger.debug("sCreateLine1={}", sCreateLine1);
		
		                    // Generate the 1st line of JSON (used for each 'update' request)
		                    HashMap<String, String> innerMapUpdate = new HashMap<String, String>();
		                    innerMapUpdate.put("_index", ES_INDEX_NAME);
		                    innerMapUpdate.put("_type",  ES_MAPPING_NAME);
		                    innerMapUpdate.put("_id",    "?");
		                    HashMap<String, Object> outerMapUpdate = new HashMap<String, Object>();
		                    outerMapUpdate.put("update", innerMapUpdate);
		                    final String sUpdateLine1 = gson.toJson(outerMapUpdate);
		                    logger.debug("sUpdateLine1={}", sUpdateLine1);
		
		
		                    // Generate the 1st line of JSON (used for each 'delete' request)
		                    HashMap<String, String> innerMapDelete = new HashMap<String, String>();
		                    innerMapDelete.put("_index", ES_INDEX_NAME);
		                    innerMapDelete.put("_type",  ES_MAPPING_NAME);
		                    innerMapDelete.put("_id",    "?");
		                    HashMap<String, Object> outerMapDelete = new HashMap<String, Object>();
		                    outerMapDelete.put("delete", innerMapDelete);
		                    final String sDeleteLine1 = gson.toJson(outerMapDelete);
		                    logger.debug("sDeleteLine1={}", sDeleteLine1);
		
		
		
		                    // Loop through the read-only recordset
		                    int i=0;
		                    while (rs.next())
		                    {
		                        i++;
		                        String sChangeId = rs.getString("id");
		                        String sDocid = rs.getString("docid");
		                        String sChangeType = rs.getString("type");
		
		                        changeIds.add(sChangeId);
		
		                        if (sChangeType.equalsIgnoreCase("CREATE"))
		                        {
		                            // Generate a HashMap with the values to index in ElasticSearch
		                            HashMap<String, String> data = new HashMap<String, String>();
		                            data.put("ingestDate", "20160214");
		                            data.put("createDate", "20150701");
		                            data.put("source", "log format1");
		                            data.put("title", "This is the record " + i);
		                            data.put("description", "this is the description for this record");
		
		                            // Append 2 lines of JSON:
		                            //  1) the 1st line tells ES that this is a 'create request'
		                            //  2) the 2nd line tells ES the data for this request
		                            sbJsonBulkRequest.append(sCreateLine1)
		                                             .append("\n")
		                                             .append(gson.toJson(data))
		                                             .append("\n");
		                        }
		                        else if (sChangeType.equalsIgnoreCase("UPDATE"))
		                        {
		                            // Update an existing document within ElasticSearch
		
		                            HashMap<String, String> data = new HashMap<String, String>();
		                            data.put("ingestDate", "20160214");
		                            data.put("createDate", "20150701");
		                            data.put("source", "log format1");
		                            data.put("title", "This is the updated record " + i);
		                            data.put("description", "this is the updated description for this record");
		
		                            // Append 2 lines of JSON:
		                            //  1) the 1st line tells ES that this is a 'create request'
		                            //  2) the 2nd line tells ES the data for this request
		                            sbJsonBulkRequest.append(sUpdateLine1.replace("?", sDocid))
		                                             .append("\n")
		                                             .append(gson.toJson(data))
		                                             .append("\n");
		
		                        }
		                        else if (sChangeType.equalsIgnoreCase("DELETE"))
		                        {
		                            // Delete an existing document within ElasticSearch
		                            sbJsonBulkRequest.append(sDeleteLine1.replace("?", sDocid))
		                                             .append("\n");
		                        }
		
		
		                        bChangeRecordsWereFound = true;
		                    }
		
		
		                    if (bChangeRecordsWereFound)
		                    {
		
		                        // Add one more carriage return
		                        sbJsonBulkRequest.append("\n");
		
		
		                        // Delete all of these records from the changes table  [once the SQL transaction is committed]
		                        String sCsvDocids = String.join(",", changeIds);
		                        String sSqlToDeleteRecords =
		                                "delete from changes " +
		                                        "where id IN (" + sCsvDocids + ") ";
		                        jt.update(sSqlToDeleteRecords);
		
		
		                        // Make a large POST call to index all of the records
		                        Response response = httpClient.preparePost(ES_URL + "/_bulk")
		                                .setHeader("accept", "application/json")
		                                .setBody(sbJsonBulkRequest.toString())
		                                .execute()
		                                .get();
		
		                        if (response.getStatusCode() != 200)
		                        {
		                            // ElasticSearch returned a non-200 status response -- that's bad
		                            throw new RuntimeException("Critical Error in Sync Program:  I got a non-200 status code of " + response.getStatusCode() + ".  The error is " + response.getStatusText());
		                        }
		                        else
		                        {
		                            // ElasticSearch returned a 200 status
		                            // -- now, Verify that I got no errors
		                            String sJsonResponse = response.getResponseBody().toString();
		                            JsonElement jelement = new JsonParser().parse(sJsonResponse);
		                            JsonObject jobject = jelement.getAsJsonObject();
		                            Boolean bErrors = jobject.get("errors").getAsBoolean();
		                            logger.debug("ElasticSearch returned errors={}", bErrors);
		
		                            if (bErrors)
		                            {
		                                throw new RuntimeException("Critical Error in Sync Program:  I got a 200 status code, but I got some errors.  Here is the returned json:  " + sJsonResponse);
		                            }
		                        }
		                    }
		
		                    // C O M M I T       T R A N S A C T I O N
		                }
		                catch(Exception e)
		                {
		                    // Rollback the transaction by calling setRollbackOnly() or by throwing a RuntimeException
		                    RuntimeException re = new RuntimeException("Exception occurred, rolling back transaction", e);
		                    re.setStackTrace(e.getStackTrace() );
		                    throw re;
		                }
		            }
		        });
		
		
		        // Close the httpClient object
		        httpClient.close();
		
		        logger.debug("main() finished");
		    }
		
		}



A L G O R I T H M
-----------------		    
		 // Our worker, scheduled to run every N minutes.
// It will read 1000 entries off the queue, issue a single batch request
// to elasticsearch, and repeat if more items are still on the queue
class QueueWorker < Worker {
  function work() {
    errorCount = 0;
    // Transactionally reserve 1000 entries at a time, but don't delete them from the queue yet
    // The entries will be only be fully deleted once the bulk operation has successfully completed
    while ((queue_entries = Queue.reserve(limit: 1000)) && records.length > 0) {
      try {
        // In this example we will build up the JSON-like body for the 
        // elasticsearch queue API request. It consists of newline separated
        // JSON documents comprising action metadata, and document values
        bulk_body = "";
        record_class = Class.named(queue_entry.record_type);
        i = 0;  
        queue_entries.each(function (queue_entry) {
          i++;
          record = record_class.find(id: queue_entry.record_id);
       
          // Note, a production ready version of this code would double-check that the document still exists
          // and would create a bulk delete request if the record was no longer present
          action_metadata = {index: {
                              _index: record_class.elasticsearch_index,
                              _type: record_class.elasticsearch_type,
                              _id: record.id}}
       
          bulk_body += "\n" unless i == 1;
          bulk_body += encode_json(action_metadata);
          bulk_body += "\n";            
          bulk_body += encode_json(record);
        });
        
        bulk_body += "\n"; // The bulk API requires termination by a newline  
        http_client.post("http://host.for.elasticsearch:9200/_bulk", body: bulk_body);
        // Now that we're sure processing has succeeded we can fully delete the queue entries
        queue_entries.delete
      } catch (StandardError ex) {
        queue_entries.unreserve;
        
        // Simply let the while loop retry up to 5 times.
        errorCount += 1
        if (errorCount >= 5) {
          throw(CannotReplicateElasticsearchError);
        } else {
          Logger.warn("Error processing elasticsearchqueue, will retry. Attempt: $errorCount", ex)
        }
      }
    }
  }
}
 
 
 
 
 
 
Part 3:  Run the program
------------------------
 1. Query against the ElasticSearch index (there should be no records)
 	a. Startup Chrome
 	b. Run the "Sense" plugin
 	c. Run this command:
 			GET /docs/_search
 			
 
 2. Run the Java Program
 
 
 
 3. Query against the ElasticSearch index (there should be a few records)
  	a. Startup Chrome
 	b. Run the "Sense" plugin
 	c. Run this command:
 			GET /docs/_search
 