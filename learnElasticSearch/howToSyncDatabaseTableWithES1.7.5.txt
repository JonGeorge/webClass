How to Sync a Postgres Database Table w/ElasticSearch 1.7.5 Index 
-----------------------------------------------------------------
The database table holds the "record of truth" and as the database table is updated,
we want to update an ElasticSearch index.  But, we do not want to do it in real-time

As you add, modify, and delete records from the database table, 
we want to *PERIODICALLY* add, modify, and delete records from the ElasticSearch index
    

NOTE:  This program will also works with an ElasticSearch 2.3.3 instance as well


Assumptions:
 A) You have an Elastic Search instance 1.7.5 up & listening at 192.168.1.157 on port 9200/9300
    [see howToInstallElasticSearch1.7.5OnCentosUsingRpm.txt]
    
 B) You have a Postgres database table up & listening at 192.168.1.157 on port 5432
    [see howToInstallPostgresOnCentOS.txt]
    



References
----------
https://qafoo.com/blog/086_how_to_synchronize_a_database_with_elastic_search.html
https://www.elastic.co/blog/found-keeping-elasticsearch-in-sync
http://dba.stackexchange.com/questions/27688/locking-issue-with-concurrent-delete-insert-in-postgresql
http://stackoverflow.com/questions/16920902/elasticsearch-java-bulk-batch-size

    
    
Part 1:  Setup the Database Table and ElasticSearch Index
----------------------------------------------------------
 1. Setup the Postgres database tables
 	a. Create this table:  documents
 	   NOTE:  The "serial" type causes a few things to happeN:
 	            a) It creates a sequence called "documents_id_seq"
 	            b) It causes the id column to use the default value of the sequence
 	                   create table documents
 	                    (
 	                   		id INT NOT NULL DEFAULT NEXTVAL('documents_id_seq'), 
 	       					...
 	       				);
 	       				
 	       				
 	
			CREATE TABLE documents
			(
			  id         serial,
			  createDate timestamp   not null,
			  ingestDate timestamp   not null,
			  title      varchar(50) not null,
			  author     varchar(50) not null,
			  summary    varchar(50) null,
			  primary key(id)
			);

 		
 	b. Create this table:  changes

			CREATE TYPE changetype AS ENUM('create','delete','update');
			CREATE TABLE changes
			(
			  id      serial,
			  docid   int not null,
		      type    changetype NOT NULL
			);

 
  	c. Add some sample records to the documents and changes tables
 			insert into documents(id, title, author, createDate, ingestDate) values(1, 'The Wizard of Oz', 'someone',                        now(), (now() - INTERVAL '30 days') );
 			insert into documents(id, title, author, createDate, ingestDate) values(2, 'Hitchhiker''s Guide to the Galaxy', 'Douglas Adams', now(), (now() - INTERVAL '10 days') );
 			insert into changes(docid, type) values(1, 'create');
 			insert into changes(docid, type) values(2, 'create');
 			
 
 
 2. Setup a mapping in ElasticSearch called docs
 	a. Startup Chrome
 	b. Startup the "Sense" plugin
 	c. Run the following commands:	
	 
		1) Delete the index called "docs" [and all records and the mapping, too!]
	    	DELETE /docs
	
	
	 	2) Create a mappings for the index called "docs"
		    PUT /docs
	    	{
		       "mappings": {
		          "record": {
		             "properties": {
		                "title": {
		                   "type": "string",
		                   "analyzer": "snowball",
		                   "copy_to": "search_text" 
		                },	                
		                "source": {
		                   "type": "string",
		                   "index": "not_analyzed"
		                },
		                "ingestDate": {
		                   "type": "date",
		                   "format": "yyyyMMdd",
		                   "index": "not_analyzed"
		                },
		                "createDate": {
		                   "type": "date",
		                   "format": "yyyyMMdd",
		                   "index": "not_analyzed"
		                },	
		                "description": {
		                   "type": "string",
		                   "analyzer": "snowball",
		                   "copy_to": "search_text" 
		                },
		                "search_text": {
		                   "type": "string",
		                   "analyzer": "standard"
		                }
		             }
		          }
		       }
		    }
 

 
		3) Add some sample records to this index
	    	POST _bulk
	    	{ "create": { "_index": "docs", "_type": "record"}}
	    	{ "title": "Record #1", "source": "log format1", "ingestDate", "20150526", "createDate": "20160124", "description": "This is the description for record #1" }
	    	{ "create": { "_index": "docs", "_type": "record"}}
	    	{ "title": "Record #2", "source": "log format2", "createDate": "02/25/2016", "description": "This is the description for record #2"}
	    	{ "create": { "_index": "docs", "_type": "record"}}
	    	{ "title": "Record #3", "source": "log format2", "createDate": "02/25/2016", "description": "This is the description for record #3"}
    
    
    	4) View all records in this index
	    	GET /docs/record/_search
    
    

Part 2:  Create this Java program that will process 1000 records from the changes table 
--------------------------------------------------------------------------------------- 
 1. Create a Java command-line project
    [see learnJava / howToCreateJavaCommandLineProgramUsingIntellijMaven.txt]
 
 
 2. Add these dependencies to your pom.xml


      <dependency>
          <!-- Used to convert java objects to JSON -->
          <groupId>com.google.code.gson</groupId>
          <artifactId>gson</artifactId>
          <version>2.8.0</version>
      </dependency>

      <dependency>
          <!-- Used for my httpClient implementation -->
          <groupId>com.ning</groupId>
          <artifactId>async-http-client</artifactId>
          <version>1.9.31</version>
      </dependency>

       <dependency>
           <!-- Spring Core -->
          <groupId>org.springframework</groupId>
          <artifactId>spring-core</artifactId>
          <version>4.1.1.RELEASE</version>
          <exclusions>
              <exclusion>
                  <groupId>commons-logging</groupId>
                  <artifactId>commons-logging</artifactId>
              </exclusion>
          </exclusions>
      </dependency>

      <dependency>
          <!-- Used to get the Spring JDBC Single Connection Data Source -->
          <groupId>org.springframework</groupId>
          <artifactId>spring-jdbc</artifactId>
          <version>4.1.1.RELEASE</version>
      </dependency>

      <dependency>
          <!-- Add the dependency for your postgreSQL driver -->
          <groupId>org.postgresql</groupId>
          <artifactId>postgresql</artifactId>
          <version>9.4.1212.jre7</version>
      </dependency>

      <dependency>
          <!-- Bridge Logging from Java Common Logging to SLF4J -->
          <groupId>org.slf4j</groupId>
          <artifactId>jcl-over-slf4j</artifactId>
          <version>1.7.5</version>
      </dependency>

      <dependency>
          <!-- Logback is our SLF4j logging implementation -->
          <groupId>ch.qos.logback</groupId>
          <artifactId>logback-classic</artifactId>
          <version>1.0.13</version>
      </dependency>
      


 3. Update your logback.xml file to look like this:
    a. Edit /src/main/resources/logback.xml
    b. Replace your logback.xml with this:
    
		<?xml version="1.0" encoding="windows-1252" ?>
		<!DOCTYPE project>
		
		<configuration debug="false">
		    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
		        <encoder>
		            <pattern>%d{MM/dd/yyyy HH:mm:ss} %-5level %c %m%n</pattern>
		        </encoder>
		    </appender>
		
		
		    <logger name="stuff" level="DEBUG" additivity="false">
		        <appender-ref ref="CONSOLE"/>
		    </logger>
		
		    <logger name="org.elasticsearch" level="INFO" additivity="false">
		        <appender-ref ref="CONSOLE"/>
		    </logger>
		
		    <logger name="org.springframework.jdbc" level="INFO" additivity="false">
		        <appender-ref ref="CONSOLE"/>
		    </logger>
		
		    <root level="INFO">
		        <appender-ref ref="CONSOLE"/>
		    </root>
		
		</configuration>
		        
		              
 
 4. Create a package called "stuff"
    a. Right-click on /src/main/java -> New -> Package
       Name:  Stuff
       
 
				
 5. Create this class:  PostgresSingleton
    a. Right-click on /src/main/java/stuff -> New -> Java Class
       Name:  PostgresSingleton
       Kind:  Class
       Press O
       
    b. Copy this to to your java class
    
		package stuff;
		
		import org.slf4j.Logger;
		import org.slf4j.LoggerFactory;
		import org.springframework.jdbc.core.JdbcTemplate;
		import org.springframework.jdbc.datasource.SingleConnectionDataSource;
		
		import javax.sql.DataSource;
		
		/**
		 * Created by adam on 10/30/2016.
		 */
		public enum PostgresSingleton
		{
		    INSTANCE;
		
		    private DataSource datasource;
		
		    /******************************************************************
		     * Private PostgresSingleton Constructor
		     * -- Do one-time initialization here
		     *******************************************************************/
		    private PostgresSingleton()
		    {
		        // The very first time this singleton is called, we will initialize the data source by attempting to run a query
		        initializeDataSourceNoException();
		    }
		
		
		    /******************************************************************
		     * initializeDataSourceNoException()
		     * -- Attempt to connect to the Postgres Data Source
		     *******************************************************************/
		    private void initializeDataSourceNoException()
		    {
		        try {
		            // Create the data source object
		            SingleConnectionDataSource sds = (SingleConnectionDataSource) new SingleConnectionDataSource();
		
		            // Initialize the data source by setting its properties
		            sds.setDriverClassName("org.postgresql.Driver");
		            sds.setUrl("jdbc:postgresql://192.168.1.157:5432/postgres");
		            sds.setUsername("postgres");
		            sds.setPassword("secret");
		
		            this.datasource = sds;
		
		            JdbcTemplate jt = new JdbcTemplate(sds);
		            final String sSql = "select now()";
		            String sCurrentDateTime = jt.queryForObject(sSql, String.class);
		        }
		        catch (Exception e)
		        {
		            // Convert the exception to a RunTime exception
		            RuntimeException re = new RuntimeException(e);
		            re.setStackTrace(e.getStackTrace());
		            throw re;
		        }
		
		    }
		
		    public DataSource getDataSource()
		    {
		        return datasource;
		    }
		}
	
		 
 6. Create this class:  SyncRecords
    a. Right-click on /src/main/java/stuff -> New -> Java Class
       Name:  SyncRecords
       Kind:  Class
       Press OK
       
       
    b. Copy this to to your java class
		    
		package stuff;
		
		import com.google.gson.*;
		import com.ning.http.client.AsyncHttpClient;
		import com.ning.http.client.Response;
		import org.slf4j.Logger;
		import org.slf4j.LoggerFactory;
		import org.springframework.jdbc.core.JdbcTemplate;
		import org.springframework.jdbc.datasource.DataSourceTransactionManager;
		import org.springframework.jdbc.support.rowset.SqlRowSet;
		import org.springframework.transaction.TransactionStatus;
		import org.springframework.transaction.support.TransactionCallbackWithoutResult;
		import org.springframework.transaction.support.TransactionTemplate;
		import org.springframework.util.StringUtils;
		
		import javax.sql.DataSource;
		
		import java.util.*;
		import java.util.regex.Pattern;
		
		/**
		 * SyncRecords
		 *
		 */
		public class SyncRecords
		{
		    public static final Logger logger = LoggerFactory.getLogger(SyncRecords.class);
		
		    private String     elasticsearchUrl = null;
		    private DataSource dataSource = null;
		    private int        totalRecordsToProcessInOneBulkRequest;
		    private int        transactionTimeoutPeriodInSecs;   // Total number of secs before transaction times out
		    private int        totalRecordsToDeleteInOneRequest;
		
		
		    /********************************************************************************************
		     * SyncRecords()   **Constructor**
		     *********************************************************************************************/
		    public SyncRecords(String aElasticSearchUrl,
		                       DataSource aDataSource,
		                       int aTotalRecordsToProcessIneOneBulkRequest,
		                       int aTransactionTimeoutPeriodInSecs,
		                       int aTotalRecordsToDeleteInOneRequest)
		    {
		        logger.debug("SyncRecords() started. aElasticSearchUrl={}", aElasticSearchUrl);
		
		        if (StringUtils.isEmpty(aElasticSearchUrl))
		        {
		            throw new RuntimeException("Critical Error in SyncRecords()  aElasticSearchUrl is null or empty.");
		        }
		        else if (aDataSource == null)
		        {
		            throw new RuntimeException("Critical Error in SyncRecords()  aDataSource is null.");
		        }
		        else if (aTotalRecordsToProcessIneOneBulkRequest <= 0)
		        {
		            throw new RuntimeException("Critical Error in SyncRecords()  aTotalRecordsToProcessIneOneBulkRequest is zero or less.  It must be a positive number.");
		        }
		
		        this.elasticsearchUrl = aElasticSearchUrl;
		        this.dataSource = aDataSource;
		        this.totalRecordsToProcessInOneBulkRequest = aTotalRecordsToProcessIneOneBulkRequest;
		        this.transactionTimeoutPeriodInSecs = aTransactionTimeoutPeriodInSecs;
		        this.totalRecordsToDeleteInOneRequest = aTotalRecordsToDeleteInOneRequest;
		    }
		
		
		    /********************************************************************************************
		     * getMapOfDetailsForDocid()
		     *
		     * Pass-in a Docid and returns a hashmap of details
		     *********************************************************************************************/
		    private HashMap<String, Object> getMapOfDetailsForDocid(String asDocid)
		    {
		        if (StringUtils.isEmpty(asDocid))
		        {
		            throw new RuntimeException("Critical Error in getMapOfDetailsForDocid():  The passed-in aDocid is empty is null.");
		        }
		
		        HashMap<String, Object> dataMap = new HashMap<String, Object>();
		
		        // Convert the passed-in string docid into a long
		        Long lDocid = new Long(asDocid);
		
		        // Run the query against the database to get all of the information for this docid
		        final String sSql = "SELECT title, author, " +
		                "to_char(createDate,'YYYYmmdd') as createDate, " +
		                "to_char(ingestDate,'YYYYmmdd') as ingestDate  " +
		                "FROM documents " +
		                "WHERE id=?";
		
		        JdbcTemplate jt = new JdbcTemplate(this.dataSource);
		        SqlRowSet rs = jt.queryForRowSet(sSql, lDocid);
		
		        if (rs.next())
		        {
		            // I found a documents record -- so put the values into this hashmap
		            dataMap.put("ingestDate", rs.getString("ingestDate"));
		            dataMap.put("createDate", rs.getString("createDate"));
		            dataMap.put("author",     rs.getString("author"));
		            dataMap.put("title",      rs.getString("title"));
		        }
		        else
		        {
		            // I did not find a record with this docid (in the database)
		            logger.warn("Warning in getMapOfDetailsForDocid():  I did not find a record for docid={}.  This should never happen.", asDocid);
		        }
		
		        return dataMap;
		    }
		
		
		    /********************************************************************************************
		     * deleteRecordsFromChangesTableWithIds()
		     *
		     * NOTE:  Delete in batches of 10,000 records at a time (to avoid sending a massive single delete call to postgess)
		     *********************************************************************************************/
		    private void deleteRecordsFromChangesTableWithIds(ArrayList<String> ids) throws Exception
		    {
		        logger.debug("deleteRecordsFromChangesTableWithIds() started  ids.size()={}  batch size={}", ids.size(), this.totalRecordsToDeleteInOneRequest);
		        long startTime = System.currentTimeMillis();
		
		        StringBuilder sbCsvIds = new StringBuilder();
		        JdbcTemplate jt = new JdbcTemplate(this.dataSource);
		        int iTotal=0;
		
		        for (int i=0; i<ids.size(); i++)
		        {
		
		            if (((i % this.totalRecordsToDeleteInOneRequest) == 0) && (i > 0))
		            {
		                // This is one batch of 10,000
		                // -- Run the SQL to delete these 10,000 records
		                sbCsvIds.deleteCharAt(sbCsvIds.length() - 1);
		
		                String sSqlToDeleteRecords =
		                        "delete from changes " +
		                                "where id IN (" + sbCsvIds.toString() + ") ";
		
		                logger.debug("Deleting changes records initiated.  iTotal={}  i={}", iTotal, i);
		                jt.update(sSqlToDeleteRecords);
		
		                // Clear the string builder
		                sbCsvIds.setLength(0);
		
		                iTotal=0;
		            }
		
		
		            String sId = ids.get(i);
		
		            sbCsvIds.append(sId)
		                    .append(",");
		
		            iTotal++;
		        }
		
		        if (sbCsvIds.length() > 0)
		        {
		            // Delete is the last batch
		
		            // Remove the last comma
		            sbCsvIds.deleteCharAt(sbCsvIds.length() -1);
		
		            String sSqlToDeleteRecords =
		                    "delete from changes " +
		                            "where id IN (" + sbCsvIds.toString() + ") ";
		
		            logger.debug("Deleting last batch of records initiated.  iTotal={}", iTotal);
		            jt.update(sSqlToDeleteRecords);
		        }
		
		        long endTime = System.currentTimeMillis();
		        logger.debug("deleteRecordsFromChangesTableWithIds() finished in {} ms.", (endTime - startTime));
		     }
		
		    /********************************************************************************************
		     * syncIndex1()
		     *********************************************************************************************/
		    public void syncIndex1() throws Exception
		    {
		        logger.debug("syncIndex1() started");
		
		        // Setup an http client object [we will use this to make the actual POST calls]
		        final AsyncHttpClient httpClient = new AsyncHttpClient();
		
		        // Setup a SQL Transaction
		        TransactionTemplate tt = new TransactionTemplate();
		        tt.setTransactionManager(new DataSourceTransactionManager(this.dataSource));
		
		        // This transaction will throw a TransactionTimedOutException after 120 seconds (causing the transaction to rollback)
		        tt.setTimeout(this.transactionTimeoutPeriodInSecs);
		
		        final String ES_INDEX_NAME   = "docs";
		        final String ES_MAPPING_NAME = "record";
		
		        tt.execute(new TransactionCallbackWithoutResult()
		        {
		            protected void doInTransactionWithoutResult(TransactionStatus aStatus)
		            {
		                try
		                {
		                    Pattern patMatchQuestionChar = Pattern.compile("[?]");
		                    int iTotalSuccessIndexOperations = 0;
		                    int iTotalFailIndexOperations = 0;
		                    StringBuilder sbJsonBulkRequest = new StringBuilder();
		                    ArrayList<String> changeIds = new ArrayList<String>();
		
		                    // Reserve 1000 entries at a time
		                    // NOTE:  If another process calls select...for update, then that other process is blocked until this transaction is finished
		                    //        If another process calls delete on these ids, then that other process is blocked until this transaction is finished
		                    //        If another process calls select...., then that other process can see the records
		                    final String sSql = "select id, docid, type " +
		                                        "from changes " +
		                                        "where id IN " +
		                                        "(select id from changes limit " + totalRecordsToProcessInOneBulkRequest + ") " +
		                                        "order by id " +
		                                        "FOR UPDATE";
		
		                    // Run the query, get a read-only recordset, and return the connection back to the JDBC connection pool
		                    JdbcTemplate jt = new JdbcTemplate(dataSource);
		                    SqlRowSet rs = jt.queryForRowSet(sSql);
		
		                    Gson gson = new Gson();
		
		                    // Generate the 1st line of JSON (used for each 'create' or 'update' request)
		                    HashMap<String, String> innerMapCreate = new HashMap<String, String>();
		                    innerMapCreate.put("_index", ES_INDEX_NAME);
		                    innerMapCreate.put("_type",  ES_MAPPING_NAME);
		                    innerMapCreate.put("_id",    "?");
		                    HashMap<String, Object> outerMapCreate = new HashMap<String, Object>();
		                    outerMapCreate.put("index", innerMapCreate);
		                    final String sCreateOrUpdateLine1 = gson.toJson(outerMapCreate);
		
		                    // Generate the 1st line of JSON (used for each 'delete' request)
		                    HashMap<String, String> innerMapDelete = new HashMap<String, String>();
		                    innerMapDelete.put("_index", ES_INDEX_NAME);
		                    innerMapDelete.put("_type",  ES_MAPPING_NAME);
		                    innerMapDelete.put("_id",    "?");
		                    HashMap<String, Object> outerMapDelete = new HashMap<String, Object>();
		                    outerMapDelete.put("delete", innerMapDelete);
		                    final String sDeleteLine1 = gson.toJson(outerMapDelete);
		
		
		                    // Loop through the read-only recordset of *changes* records
		                    int iTotalBulkIndexOperations = 0;
		                    while (rs.next())
		                    {
		                        String sChangeId = rs.getString("id");
		                        String sDocid = rs.getString("docid");
		                        String sChangeType = rs.getString("type");
		
		                        changeIds.add(sChangeId);
		
		                        if ((sChangeType.equalsIgnoreCase("CREATE")) || (sChangeType.equalsIgnoreCase("UPDATE")))
		                        {
		                            // Append the JSON needed to create or update a document within ElasticSearch
		
		                            // Generate a HashMap with the values to index in ElasticSearch
		                            HashMap<String, Object> data = getMapOfDetailsForDocid(sDocid);
		
		                            // Convert the HashMap into JSON
		                            String sJsonDetails = gson.toJson(data);
		
		                            // Replace the "?" with the docid
		                            String sCreateOrUpdateLine1WithId = patMatchQuestionChar.matcher(sCreateOrUpdateLine1).replaceFirst(sDocid);
		
		                            // Append 2 lines of JSON:
		                            //  1) the 1st line tells ES that this is a 'create request'
		                            //  2) the 2nd line tells ES the data for this request
		                            sbJsonBulkRequest.append(sCreateOrUpdateLine1WithId)
		                                             .append("\n")
		                                             .append(sJsonDetails)
		                                             .append("\n");
		
		                            iTotalBulkIndexOperations++;
		                        }
		                        else if (sChangeType.equalsIgnoreCase("DELETE"))
		                        {
		                            // Append the JSON needed to delete a document within ElasticSearch
		                            String sDeleteLine1WithId = patMatchQuestionChar.matcher(sDeleteLine1).replaceFirst(sDocid);
		
		                            // Delete an existing document within ElasticSearch
		                            sbJsonBulkRequest.append(sDeleteLine1WithId)
		                                             .append("\n");
		
		                            iTotalBulkIndexOperations++;
		                        }
		
		                        if ((iTotalBulkIndexOperations % 1000) == 0)
		                        {
		                            logger.debug("Finished preparing another 1000.  Up to {}", iTotalBulkIndexOperations);
		                        }
		
		                    }
		
		
		                    if (iTotalBulkIndexOperations == 0)
		                    {
		                        logger.debug("I found no records in the changes table.  There is nothing to index.");
		                    }
		                    else
		                    {
		                        // I have some JSON to send to ElasticSearch
		
		                        // Add one more carriage return to the JSON (so the bulk request is valid)
		                        sbJsonBulkRequest.append("\n");
		
		                        // Delete all of these records from the changes table  [once the SQL transaction is committed]
		                        deleteRecordsFromChangesTableWithIds(changeIds);
		
		                        // Make a POST call to ElasticSearch  (this performs the bulk index request)
		                        logger.debug("ElasticSearch POST initiated.  Total Bulk Operations Requested={}", iTotalBulkIndexOperations);
		                        long startTime = System.currentTimeMillis();
		                        Response response = httpClient.preparePost(elasticsearchUrl + "/_bulk")
		                                .setHeader("accept", "application/json")
		                                .setBody(sbJsonBulkRequest.toString())
		                                .execute()
		                                .get();
		                        long endTime = System.currentTimeMillis();
		                        long lTotalTimeMs = endTime - startTime;
		                        logger.debug("ElasticSearch POST finished in {} ms with status code of {}", lTotalTimeMs, response.getStatusCode());
		
		                        if (response.getStatusCode() != 200)
		                        {
		                            // ElasticSearch returned a non-200 status response -- that's bad
		                            throw new RuntimeException("Critical Error in Sync Program:  I got a non-200 status code of " + response.getStatusCode() + ".  The error is " + response.getResponseBody());
		                        }
		                        else
		                        {
		                            // ElasticSearch returned a 200 status -- that's good
		
		                            // Parse the returned JSON string and check the returned "errors" field  (hopefully it holds "false")
		                            String sJsonResponse = response.getResponseBody();
		                            JsonElement jelement = new JsonParser().parse(sJsonResponse);
		                            JsonObject jobject = jelement.getAsJsonObject();
		                            Boolean bErrors = jobject.get("errors").getAsBoolean();
		                            logger.debug("ElasticSearch returned errors={}", bErrors);
		
		                            if (bErrors == false)
		                            {
		                                // There were no errors performing this bulk operation
		                                // B U L K    O P E R A T I O N     W A S    1 0 0 %    S U C C E S S F U L
		                                logger.debug("Index operation finished successfully.  There were no errors.");
		                            }
		                            else
		                            {
		                                // B U L K    O P E R A T I O N     P A R T I A L L Y    F A I L E D
		                                // -- The "errors" flag holds true -- so one or more items *FAILED* to index
		                                logger.error("Critical Error in Sync Program:  I got a 200 status code, but I got some errors.  Here is the returned json:  " + sJsonResponse);
		
		                                // The Response object's JSON holds information under "create", "index", or "delete" keys
		                                // -- So, we need to look under all of them to find out which "create" operations worked, which "index" operations worked, and which "delete" operations worked
		                                List<String> operationNames = Arrays.asList("create","index","delete");
		
		                                JsonArray items = jobject.getAsJsonArray("items");
		                                for (JsonElement item: items)
		                                {
		                                    JsonObject jitem = item.getAsJsonObject();
		
		                                    for (String sIndexOperationName : operationNames)
		                                    {
		                                        if (jitem.has(sIndexOperationName))
		                                        {
		                                            JsonObject jitemDetails = jitem.get(sIndexOperationName).getAsJsonObject();
		
		                                            int iStatus = jitemDetails.get("status").getAsInt();
		                                            String sDocid = jitemDetails.get("_id").getAsString();
		                                            logger.debug("{} operation on docid={} with status={}", sIndexOperationName, sDocid, iStatus);
		
		                                            if ((iStatus == 200) || (iStatus == 201))
		                                            {
		                                                // Out of the N index operations, this one operation succeeded
		                                                logger.error("ElasticSearch {} operation succeeded for this docid: ", sIndexOperationName, sDocid);
		
		                                                iTotalSuccessIndexOperations++;
		                                            }
		                                            else
		                                            {
		                                                // This item failed to index
		                                                String sErrorMesg = jitemDetails.get("error").getAsString();
		                                                logger.error("ElasticSearch {} operation failed for this docid {}.  Error is {}", sIndexOperationName, sDocid, sErrorMesg);
		                                                iTotalFailIndexOperations++;
		                                            }
		                                        }
		
		                                    }  // end of looping thru "create", "index", and "delete" strings
		                                }
		
		                                logger.warn("WARNING: iTotalSuccessIndexOperations={}  iTotalFailIndexOperations={}", iTotalSuccessIndexOperations, iTotalFailIndexOperations);
		                            }
		                        }
		                    }
		
		
		                    // C O M M I T      S Q L      T R A N S A C T I O N
		                }
		                catch(Exception e)
		                {
		                    // Rollback the transaction by calling setRollbackOnly() or by throwing a RuntimeException
		                    RuntimeException re = new RuntimeException("Exception occurred, rolling back transaction", e);
		                    re.setStackTrace(e.getStackTrace() );
		                    throw re;
		                }
		                finally
		                {
		                    // Close the httpClient object
		                    httpClient.close();
		                }
		            }
		        });
		
		
		        logger.debug("syncIndex1() finished.");
		    }
		
		}



 7. Create this class:  App
    a. Right-click on /src/main/java/stuff -> New -> Java Class
       Name:  App
       Kind:  Class
       Press OK
       
       
    b. Copy this to to your java class

		package stuff;
		
		import org.slf4j.Logger;
		import org.slf4j.LoggerFactory;
		import org.springframework.jdbc.core.JdbcTemplate;
		import org.springframework.jdbc.datasource.DataSourceTransactionManager;
		import org.springframework.transaction.TransactionStatus;
		import org.springframework.transaction.support.TransactionCallbackWithoutResult;
		import org.springframework.transaction.support.TransactionTemplate;
		
		import javax.sql.DataSource;
		
		/**
		 * Created by adam on 12/28/2016.
		 */
		public class App
		{
		    private static final Logger logger = LoggerFactory.getLogger(App.class);
		
		    /********************************************************************************************
		     * main()
		     *********************************************************************************************/
		    public static void main(String[] args) throws Exception
		    {
		        logger.debug("main() started");
		
		        long startTime = System.currentTimeMillis();
		
		
		        final String ES_URL                          =  "http://192.168.1.165:9200";
		        final int    TOTAL_RECORDS_TO_INDEX_AT_ONCE =   50000;
		        final int    TRANSACTION_TIMEOUT_IN_SECS =        900;   // Transaction times out after 900 secds (or 15 minutes)
		        final int    TOTAL_RECORDS_TO_DELETE_AT_ONCE =  10000;
		
		        // Connect to the Postgres DataSource instance and return the data source object
		        DataSource datasource = PostgresSingleton.INSTANCE.getDataSource();
		
		
		        // Create 50,000 document and changes requests
		//        createRequests(datasource);
		
		        // Create a SyncRecords object with the passed-in ElasticSearchUrl and data source
		        SyncRecords syncRecords = new SyncRecords(ES_URL, datasource, TOTAL_RECORDS_TO_INDEX_AT_ONCE, TRANSACTION_TIMEOUT_IN_SECS, TOTAL_RECORDS_TO_DELETE_AT_ONCE);
		
		        // Start syncing the records
		        syncRecords.syncIndex1();
		
		        long finishTime = System.currentTimeMillis();
		        logger.debug("main() finished in {} ms.", (finishTime - startTime));
		    }
		
		
		
		    /********************************************************************************************
		     * createRequests()
		     *********************************************************************************************/
		    private static void createRequests(final DataSource aDataSource) throws Exception
		    {
		        // Setup a SQL Transaction
		        TransactionTemplate tt = new TransactionTemplate();
		        tt.setTransactionManager(new DataSourceTransactionManager(aDataSource));
		
		        // This transaction will throw a TransactionTimedOutException after 1200 seconds (causing the transaction to rollback)
		        tt.setTimeout(1800);
		
		        tt.execute(new TransactionCallbackWithoutResult()
		        {
		            @Override
		            protected void doInTransactionWithoutResult(TransactionStatus transactionStatus) {
		                int i = 1;
		
		                final String sSqlInsertDocuments = "insert into documents(id, title, author, createDate, ingestDate) " +
		                        "values(?, ?, ?, now(), now())";
		
		
		                final String sSqlInsertChanges = "insert into changes(docid, type) values(?, CAST(? AS changetype))";
		
		                JdbcTemplate jt = new JdbcTemplate(aDataSource);
		
		                for (int docid = 3000000; docid <= 3800000; docid++)
		                {
		                    String sTitle = "title for " + docid;
		                    String sAuthor = "author for " + docid;
		                    jt.update(sSqlInsertDocuments, docid, sTitle, sAuthor);
		
		                    jt.update(sSqlInsertChanges, docid, "create");
		
		                    if ((docid % 10000) == 0)
		                    {
		                        logger.debug("Inserted another 10000.  Up to {}", docid);
		                    }
		                }
		
		                // C O M M I T
		                logger.debug("committing operation.");
		            }
		        });
		    }
		
		}


 
 
 
Part 3:  Run the program
------------------------
 1. Insert some records into the documents and changes tables
		insert into documents(id, title, author, createDate, ingestDate) values(3, 'The Wizard of Oz', 'someone',                        now(), (now() - INTERVAL '30 days') );
		insert into documents(id, title, author, createDate, ingestDate) values(4, 'Hitchhiker''s Guide to the Galaxy', 'Douglas Adams', now(), (now() - INTERVAL '10 days') );
		insert into changes(docid, type) values(3, 'create');
		insert into changes(docid, type) values(4, 'create');


 2. Query against the ElasticSearch index (there should be no records)
 	a. Startup Chrome
 	b. Run the "Sense" plugin
 	c. Run this command:
 			GET /docs/_search
 
 			
 3. Run the Java Program
 
 
 4. Query against the ElasticSearch index (there should be a few records)
  	a. Startup Chrome
 	b. Run the "Sense" plugin
 	c. Run this command:
 			GET /docs/_search
 
 

Performance Times
-----------------
These performance runs were using Virtual Box with 12 GB of RAM to run Postgres and ElasticSearch
NOTE:  All times are in seconds

  Total Change Records   Delete-batch-size    ES-Post Time       Delete-Records Time   Total Time (in secs)
     200,000                 5,000                                                        480 
     200,000                10,000                                                        328 
     100,000                10,000                3.1                  101                170  
     100,000                20,000                2.1                   59                124                             
     100,000                50,000                2.5                  101                175                              
     100,000                25,000                2.2                   59                129                    
      50,000                10,000                1.2                   14                 45
      25,000                10,000                0.7                   35                 48
      12,500                10,000                0.3                   17                 22
      10,000                10,000                0.4                   13                 18
       5,000                10,000                0.1                    6                  8
       2,500                10,000                0.1                    3                  4
       
 Here are some thoughts on the performance:
  A) Running postgres Deletes is expensive
  B) ElasticSearch POST times are crazy fast
  C) When you index 200,000 records, you need to run 200,000 queries -- that adds up
  D) It takes a *MUCH* more time to generate the JSON (to post to ES) then it takes for ES to process it
  
 
 
 
 
A L G O R I T H M
-----------------		    
// Our worker, scheduled to run every N minutes.
// It will read 1000 entries off the queue, issue a single batch request
// to elasticsearch, and repeat if more items are still on the queue
class QueueWorker < Worker
 {
  function work()
   {
    errorCount = 0;
    // Transactionally reserve 1000 entries at a time, but don't delete them from the queue yet
    // The entries will be only be fully deleted once the bulk operation has successfully completed
    while ((queue_entries = Queue.reserve(limit: 1000)) && records.length > 0) {
      try {
        // In this example we will build up the JSON-like body for the 
        // elasticsearch queue API request. It consists of newline separated
        // JSON documents comprising action metadata, and document values
        bulk_body = "";
        record_class = Class.named(queue_entry.record_type);
        i = 0;  
        queue_entries.each(function (queue_entry) {
          i++;
          record = record_class.find(id: queue_entry.record_id);
       
          // Note, a production ready version of this code would double-check that the document still exists
          // and would create a bulk delete request if the record was no longer present
          action_metadata = {index: {
                              _index: record_class.elasticsearch_index,
                              _type: record_class.elasticsearch_type,
                              _id: record.id}}
       
          bulk_body += "\n" unless i == 1;
          bulk_body += encode_json(action_metadata);
          bulk_body += "\n";            
          bulk_body += encode_json(record);
        });
        
        bulk_body += "\n"; // The bulk API requires termination by a newline  
        http_client.post("http://host.for.elasticsearch:9200/_bulk", body: bulk_body);
        // Now that we're sure processing has succeeded we can fully delete the queue entries
        queue_entries.delete
      } 
      catch (StandardError ex) 
      {
        queue_entries.unreserve;
        
        // Simply let the while loop retry up to 5 times.
        errorCount += 1
        if (errorCount >= 5) 
        {
          throw(CannotReplicateElasticsearchError);
        } 
        else
         {
          Logg
          er.warn("Error processing elasticsearchqueue, will retry. Attempt: $errorCount", ex)
        }
      }
    }
  }
}
 
 