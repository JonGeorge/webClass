Lesson:  RDD Basics
-------------------

An RDD in Spark is an immutable distributed collection of objects.  

Each RDD is split into multiple partitions (which may be computed on different notes of a cluster)

RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.


How to Create RDD
-----------------
Users create RDD in two ways:
 1) Loading an external dataset -- e.g., sparkContext.textFile()
 2) Distributing a collection of objects (list or set) in their driver programs
 
 
RDD Operations
--------------
Once created, RDDs offer two types of operations:
 1) Transformations -- create a new RDD from a previous RDD 
                       They are lazily evaluated -- e.g., when we call map(), the operation is not immediatley performed
                       Instead, Spark internally records metadata to indicate that this operation is requested
    examples:   
      map()
      filter()
    
    
      JavaRDD<String> inputRDD = sc.textFile("log.txt");
      JavaRDD<String> errorsRDD = inputRDD.filter( new Function<String, Boolean() {
         public Boolean call(String x)
          {
             return x.contains("error");
          }
      });


    
 2) Actions -- compute a result based on an RDD and 
               either (a) return the result to the driver program *or*
                      (b) save the result to an external storage (HDFS) 
    examples:                  
      count()
      first()
      take()
      
      // Take() retrieves a small number of elements in the RDD at the driver program
      for (String sLine:  badLinesRDD.take(10))
      {
         System.out.println(sLine)
      }
    
Transformations and actions are different because of the way Spark computes RDDs.
Although you can define new RDDs any time, Spark computes them only in a **lazy** fashion
  -- i.e., the *first* time they are used, that is when they are computed
  

RDDs are (by default) recomputed each time you run an action on them.
If you would like to reuse an RDD in multiple actions, you tell Spark to persist it using RDD.persist()


Every Spark program and shell session works as follows
 1. Create some input RDDs from external data
 2. Transform them to define new RDDs using transformations like filter()
 3. Ask Spark to persist() any intermediate RDDs that will need to be reused
 4. Launch actions such as count() and first() to kick off a parallel computation
    [which is then optimized and executed by Spark]
    
    
    
JavaRDD<String> lines = sc.textFile("/path/to/file");
   
   

Although transformations are lazy, you can force Spark to execute them by running an action -- e.g., count()

 
              